{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.patches as patches\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import pairwise_distances, silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "import leidenalg\n",
    "import igraph as ig\n",
    "import itertools\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.patches as mpatches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip show sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT full feature (only comparing between datasets)\n",
    "\n",
    "#RCC = pd.read_csv(\"/home/larissa/Documents/Masterarbeit/RCC_results/liana_log_norm/no_subset_glom_removed/T_lr_ready.csv\", index_col=0)\n",
    "#MF = pd.read_csv(\"/home/larissa/Documents/Masterarbeit/MF_results/liana/all_celltypes/MF_lr_ready.csv\", index_col=0)\n",
    "\n",
    "#RCC = pd.read_csv(\"D:\\\\studium\\\\Masterarbeit\\\\RCC_results\\\\liana_log_norm\\\\no_subset_glom_removed\\\\T_lr_ready.csv\", index_col=0)\n",
    "#MF = pd.read_csv(\"D:\\\\studium\\\\Masterarbeit\\\\MF_results\\\\liana\\\\all_celltypes\\\\MF_lr_ready.csv\", index_col=0)\n",
    "\n",
    "RCC = pd.read_csv(\"C:\\\\Users\\\\laris\\\\Documents\\\\Studium\\\\Masterarbeit\\\\RCC_results\\\\liana_log_norm\\\\no_subset_glom_removed\\\\T_lr_ready.csv\", index_col = 0)\n",
    "MF = pd.read_csv(\"C:\\\\Users\\\\laris\\\\Documents\\\\Studium\\\\Masterarbeit\\\\MF_results\\\\liana\\\\all_celltypes\\\\MF_lr_ready.csv\", index_col = 0)\n",
    "\n",
    "RCC[\"lr_pair\"] = RCC[\"gene_A\"] + \"_\" + RCC[\"gene_B\"]\n",
    "MF[\"lr_pair\"] = MF[\"gene_A\"] + \"_\" + MF[\"gene_B\"]\n",
    "\n",
    "RCC[\"cluster_pair\"] = RCC[\"source\"] + \"@\" + RCC[\"target\"]\n",
    "MF[\"cluster_pair\"] = MF[\"source\"] + \"@\" + MF[\"target\"]\n",
    "\n",
    "RCC[\"lr_means_normed\"] = (RCC[\"lr_means\"]-RCC[\"lr_means\"].min())/(RCC[\"lr_means\"].max()-RCC[\"lr_means\"].min())\n",
    "MF[\"lr_means_normed\"] = (MF[\"lr_means\"]-MF[\"lr_means\"].min())/(MF[\"lr_means\"].max()-MF[\"lr_means\"].min())\n",
    "\n",
    "#lr_threshold\n",
    "RCC_filtered = RCC[(RCC[\"lr_means\"] >= 0.0) | (RCC[\"lr_means\"] <=  -abs(0.0))]\n",
    "MF_filtered = MF[(MF[\"lr_means\"] >= 0.0) | (MF[\"lr_means\"] <= -abs(0.0))]\n",
    "\n",
    "RCC_grouped = (RCC_filtered.groupby(\"cluster_pair\")\n",
    "               .apply(lambda df: dict(zip(df[\"lr_pair\"], df[\"lr_means_normed\"])))\n",
    "               .to_dict())\n",
    "\n",
    "MF_grouped = (MF_filtered.groupby(\"cluster_pair\")\n",
    "              .apply(lambda df: dict(zip(df[\"lr_pair\"], df[\"lr_means_normed\"])))\n",
    "              .to_dict())\n",
    "\n",
    "# remove cell pairs that only have 5 or less unique LR pairs \n",
    "RCC_grouped = {k: v for k, v in RCC_grouped.items() if len(v) >= 5}\n",
    "MF_grouped = {k: v for k, v in MF_grouped.items() if len(v) >= 5}\n",
    "\n",
    "def weighted_jaccard_dict(dict1, dict2):\n",
    "    all_keys = set(dict1.keys()).union(dict2.keys())\n",
    "    vec1 = np.array([dict1.get(i, 0.0) for i in all_keys])\n",
    "    vec2 = np.array([dict2.get(i, 0.0) for i in all_keys])\n",
    "    return np.minimum(vec1, vec2).sum() / np.maximum(vec1, vec2).sum() if np.maximum(vec1, vec2).sum() != 0 else 0.0\n",
    "\n",
    "# pairwise similarity\n",
    "results = []\n",
    "for MF_cluster, MF_dict in MF_grouped.items():\n",
    "    for RCC_cluster, RCC_dict in RCC_grouped.items():\n",
    "        score = weighted_jaccard_dict(MF_dict, RCC_dict)\n",
    "        results.append((MF_cluster, RCC_cluster, score))\n",
    "\n",
    "jaccard_df_comp = pd.DataFrame(results, columns=[\"MF_cluster\", \"RCC_cluster\", \"weighted_jaccard\"])\n",
    "#jaccard_df.to_csv(\"jaccard_score_weighted_cluster_pairs_thresholded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RCC_filtered.groupby(\"cluster_pair\")[\"lr_pair\"].size().hist(bins=200, grid = False)\n",
    "#plt.xticks(ticks=[0, 10, 20,  30, 40, 50, 60, 70, 80, 85, 90, 95, 100],)\n",
    "plt.xlabel(\"Amount of unique LR pairs in one cellpair []\")\n",
    "plt.ylabel(\"Number of cellpairs with this number of unique LR pairs []\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RCC_filtered.groupby(\"cluster_pair\")[\"lr_pair\"].size().hist(bins=200, grid = False)\n",
    "plt.xlim(0, 25)\n",
    "plt.xlabel(\"Amount of unique LR pairs in one cellpair []\")\n",
    "plt.ylabel(\"Number of cellpairs with this number of unique LR pairs []\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((RCC_filtered.groupby(\"cluster_pair\")[\"lr_pair\"].size() < 5).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MF_filtered.groupby(\"cluster_pair\")[\"lr_pair\"].size().hist(bins=200, grid = False)\n",
    "#plt.xticks(ticks=[0, 10, 20,  30, 40, 50, 60, 70, 80, 85, 90, 95, 100],)\n",
    "plt.xlabel(\"Amount of unique LR pairs in one cellpair []\")\n",
    "plt.ylabel(\"Number of cellpairs with this number of unique LR pairs []\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MF_filtered.groupby(\"cluster_pair\")[\"lr_pair\"].size().hist(bins=200, grid = False)\n",
    "plt.xlim(0, 25)\n",
    "plt.xlabel(\"Amount of unique LR pairs in one cellpair\")\n",
    "plt.ylabel(\"Number of cellpairs with this number of unique LR pairs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(MF_filtered.groupby(\"cluster_pair\")[\"lr_pair\"].size() < 5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_df_comp_dist = 1 - jaccard_df_comp[\"weighted_jaccard\"]\n",
    "ax = sns.violinplot(jaccard_df_comp, inner= \"quart\")\n",
    "quant = np.quantile(jaccard_df_comp[\"weighted_jaccard\"], 0.75)\n",
    "print(quant)\n",
    "jaccard_df_quant = jaccard_df_comp[jaccard_df_comp[\"weighted_jaccard\"] > quant]\n",
    "ax.set_xticklabels([])\n",
    "plt.xlabel(\"Weighted Jaccard Similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full feature\n",
    "\n",
    "#RCC = pd.read_csv(\"/home/larissa/Documents/Masterarbeit/RCC_results/liana_log_norm/no_subsets/T_lr_ready.csv\", index_col=0)\n",
    "#MF = pd.read_csv(\"/home/larissa/Documents/Masterarbeit/MF_results/liana/all_celltypes/MF_lr_ready.csv\", index_col=0)\n",
    "\n",
    "#RCC = pd.read_csv(\"D:\\\\studium\\\\Masterarbeit\\\\RCC_results\\\\liana_log_norm\\\\no_subsets\\\\T_lr_ready.csv\", index_col=0)\n",
    "#MF = pd.read_csv(\"D:\\\\studium\\\\Masterarbeit\\\\MF_results\\\\liana\\\\all_celltypes\\\\MF_lr_ready.csv\", index_col=0)\n",
    "\n",
    "RCC = pd.read_csv(\"C:\\\\Users\\\\laris\\\\Documents\\\\Studium\\\\Masterarbeit\\\\RCC_results\\\\liana_log_norm\\\\no_subset_glom_removed\\\\T_lr_ready.csv\", index_col = 0)\n",
    "MF = pd.read_csv(\"C:\\\\Users\\\\laris\\\\Documents\\\\Studium\\\\Masterarbeit\\\\MF_results\\\\liana\\\\all_celltypes\\\\MF_lr_ready.csv\", index_col = 0)\n",
    "\n",
    "RCC[\"lr_pair\"] = RCC[\"gene_A\"] + \"_\" + RCC[\"gene_B\"]\n",
    "MF[\"lr_pair\"] = MF[\"gene_A\"] + \"_\" + MF[\"gene_B\"]\n",
    "\n",
    "RCC[\"cluster_pair\"] = RCC[\"source\"] + \"@\" + RCC[\"target\"]\n",
    "MF[\"cluster_pair\"] = MF[\"source\"] + \"@\" + MF[\"target\"]\n",
    "\n",
    "#lr_threshold = 0.0\n",
    "RCC_filtered = RCC[(RCC[\"lr_means\"] >= 0.0) | (RCC[\"lr_means\"] <=  -abs(0.0))]\n",
    "MF_filtered = MF[(MF[\"lr_means\"] >= 0.0) | (MF[\"lr_means\"] <= -abs(0.0))]\n",
    "\n",
    "RCC_grouped = (RCC_filtered.groupby(\"cluster_pair\")\n",
    "               .apply(lambda df: dict(zip(df[\"lr_pair\"], df[\"lr_means\"])))\n",
    "               .to_dict())\n",
    "\n",
    "MF_grouped = (MF_filtered.groupby(\"cluster_pair\")\n",
    "              .apply(lambda df: dict(zip(df[\"lr_pair\"], df[\"lr_means\"])))\n",
    "              .to_dict())\n",
    "\n",
    "def weighted_jaccard_dict(dict1, dict2):\n",
    "    all_keys = set(dict1.keys()).union(dict2.keys())\n",
    "    vec1 = np.array([dict1.get(i, 0.0) for i in all_keys])\n",
    "    vec2 = np.array([dict2.get(i, 0.0) for i in all_keys])\n",
    "    return np.minimum(vec1, vec2).sum() / np.maximum(vec1, vec2).sum() if np.maximum(vec1, vec2).sum() != 0 else 0.0\n",
    "\n",
    "# pairwise similarity\n",
    "results = []\n",
    "for MF_cluster, MF_dict in MF_grouped.items():\n",
    "    for RCC_cluster, RCC_dict in RCC_grouped.items():\n",
    "        score = weighted_jaccard_dict(MF_dict, RCC_dict)\n",
    "        results.append((MF_cluster, RCC_cluster, score))\n",
    "        results.append((RCC_cluster, MF_cluster, score))\n",
    "\n",
    "for MF_cluster, MF_dict in MF_grouped.items():\n",
    "    for MF_cluster2, MF_dict2 in MF_grouped.items():\n",
    "        score = weighted_jaccard_dict(MF_dict, MF_dict2)\n",
    "        results.append((MF_cluster, MF_cluster2, score))\n",
    "\n",
    "for RCC_cluster, RCC_dict in RCC_grouped.items():\n",
    "    for RCC_cluster2, RCC_dict2 in RCC_grouped.items():\n",
    "        score = weighted_jaccard_dict(RCC_dict, RCC_dict2)\n",
    "        results.append((RCC_cluster, RCC_cluster2, score))\n",
    "\n",
    "\n",
    "\n",
    "jaccard_df = pd.DataFrame(results, columns=[\"cluster_pair_1\", \"cluster_pair_2\", \"weighted_jaccard\"])\n",
    "#jaccard_df.to_csv(\"jaccard_score_weighted_cluster_panirs_thresholded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full feature\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "#same clusters are not 1 but do they need to be for visualization? no\n",
    "\n",
    "# pivot to square matrix\n",
    "\n",
    "sim_matrix = jaccard_df.pivot(\n",
    "    index=\"cluster_pair_1\", columns=\"cluster_pair_2\", values=\"weighted_jaccard\"\n",
    ")\n",
    "\n",
    "dist_matrix = 1 - sim_matrix\n",
    "\n",
    "#flatten to condensed distance matrix\n",
    "condensed_dist = squareform(dist_matrix.values, checks=False)\n",
    "\n",
    "# hierarchical clustering \n",
    "linkage_matrix = linkage(condensed_dist, method='average')\n",
    "\n",
    "clust_map = sns.clustermap(sim_matrix, row_linkage=linkage_matrix, col_linkage=linkage_matrix, row_cluster=True, col_cluster=True, figsize=(200,200), cmap=\"viridis\")\n",
    "#sim_matrix.to_csv(\"sim_matrix_jaccard.csv\")\n",
    "clust_map.savefig(\"jaccard_score_dist_linkage_full_feature_average_linkage.png\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for symmetrical matrix with 0 on diagonal\n",
    "\n",
    "labels = KMeans(n_clusters=10, random_state=42).fit_predict(dist_matrix)\n",
    "\n",
    "# Silhouette score\n",
    "score = silhouette_score(dist_matrix, labels, metric='precomputed')\n",
    "print(\"Silhouette score (1 - correlation):\", score)\n",
    "\n",
    "labels = AgglomerativeClustering(n_clusters=10, metric='precomputed', linkage='average').fit_predict(dist_matrix)\n",
    "\n",
    "score = silhouette_score(dist_matrix, labels, metric='precomputed')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT full feature (sparse matrix)\n",
    "\n",
    "#same clusters are not 1 but do they need to be for visualization? no\n",
    "\n",
    "# pivot to square matrix\n",
    "\n",
    "mat1 = jaccard_df_comp.pivot(\n",
    "    index=\"RCC_cluster\", columns=\"MF_cluster\", values=\"weighted_jaccard\"\n",
    ")\n",
    "mat2 = jaccard_df_comp.pivot(\n",
    "    index=\"MF_cluster\", columns=\"RCC_cluster\", values=\"weighted_jaccard\"\n",
    ")\n",
    "\n",
    "mat3 = pd.concat([mat1, mat2])\n",
    "#mat3 = mat3.sort_index(axis=0).sort_index(axis=1)\n",
    "mat3 = mat3.fillna(0)\n",
    "dist_matrix = 1 - mat3\n",
    "\n",
    "\n",
    "#flatten to condensed distance matrix\n",
    "condensed_dist = squareform(dist_matrix.values, checks=False)\n",
    "\n",
    "# hierarchical clustering \n",
    "linkage_matrix = linkage(condensed_dist, method='average')\n",
    "\n",
    "clust_map = sns.clustermap(mat3, row_linkage=linkage_matrix, col_linkage=linkage_matrix, row_cluster=True, col_cluster=True, figsize=(100,100), cmap=\"viridis\")\n",
    "#sim_matrix.to_csv(\"sim_matrix_jaccard.csv\")\n",
    "clust_map.savefig(\"jaccard_score_dist_linkage_sparse_mat_average_linkage.png\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.fill_diagonal(dist_matrix, 0)\n",
    "\n",
    "n_samples = dist_matrix.shape[0]\n",
    "\n",
    "best_score = -1\n",
    "best_k = None\n",
    "best_labels = None\n",
    "\n",
    "for k in range(2, min(100, n_samples)):  # test cluster counts from 2 to 9\n",
    "    labels = fcluster(Z, t=k, criterion='maxclust')\n",
    "    score = silhouette_score(linkage_matrix, labels, metric='precomputed')\n",
    "    print(f\"Clusters: {k}, Silhouette score: {score:.4f}\")\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_k = k\n",
    "        best_labels = labels\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Output best result\n",
    "# -----------------------------\n",
    "print(\"\\nBest number of clusters:\", best_k)\n",
    "print(\"Best silhouette score:\", best_score)\n",
    "print(\"Cluster labels:\", best_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatrmap_df = jaccard_df_comp.pivot(\n",
    "    index=\"RCC_cluster\", columns=\"MF_cluster\", values=\"weighted_jaccard\"\n",
    ")\n",
    "\n",
    "heatrmap_df = 1- heatrmap_df\n",
    "clust = sns.clustermap(heatrmap_df, figsize=(30, 30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(jaccard_df[\"weighted_jaccard\"])\n",
    "plt.xlim(-0.05, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns: all RCC MF interactions\n",
    "#rows LR pairs\n",
    "#values: LR score\n",
    "\n",
    "RCC[\"lr_pair\"] = RCC[\"gene_A\"] + \"_\" + RCC[\"gene_B\"]\n",
    "MF[\"lr_pair\"] = MF[\"gene_A\"] + \"_\" + MF[\"gene_B\"]\n",
    "\n",
    "RCC[\"cluster_pair\"] = RCC[\"source\"] + \"@\" + RCC[\"target\"]\n",
    "MF[\"cluster_pair\"] = MF[\"source\"] + \"@\" + MF[\"target\"]\n",
    "\n",
    "RCC_piv = RCC.pivot(values=\"MeanLR\", columns=\"cluster_pair\", index=\"lr_pair\")\n",
    "MF_piv = MF.pivot(values=\"MeanLR\", columns=\"cluster_pair\", index=\"lr_pair\")\n",
    "\n",
    "merged_df = RCC_piv.merge(MF_piv, left_index=True, right_index=True)\n",
    "merged_df.fillna(0, inplace=True)\n",
    "merged_df.to_csv(\"sparse_df_LRs_clusterpairs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster hierarchically\n",
    "\n",
    "heatrmap_df = jaccard_df_comp.pivot(index=\"RCC_cluster\", columns=\"MF_cluster\", values=\"weighted_jaccard\")\n",
    "heatrmap_df = 1 - heatrmap_df\n",
    "\n",
    "g = sns.clustermap(heatrmap_df, method='average', figsize = (30,30), yticklabels=0, xticklabels=0, \n",
    "                  cbar_pos=(0.12, 0.82, 0.03, 0.15))\n",
    "\n",
    "g.ax_heatmap.set_xlabel(\"RCC cluster\", fontsize=15)\n",
    "g.ax_heatmap.set_ylabel(\"MF cluster\", fontsize=15)\n",
    "cbar = g.ax_heatmap.collections[0].colorbar\n",
    "cbar.set_label(\"Jaccard distance\", fontsize=15)\n",
    "cbar.ax.yaxis.set_label_position(\"left\")\n",
    "\n",
    "row_order = g.dendrogram_row.reordered_ind\n",
    "col_order = g.dendrogram_col.reordered_ind\n",
    "reordered_rows = [heatrmap_df.index[i] for i in row_order]\n",
    "reordered_cols = [heatrmap_df.columns[i] for i in col_order]\n",
    "\n",
    "\n",
    "#define dendrogram clustering threshold\n",
    "row_clusters = fcluster(g.dendrogram_row.linkage, t=11, criterion='maxclust')\n",
    "col_clusters = fcluster(g.dendrogram_col.linkage, t=12, criterion='maxclust')\n",
    "\n",
    "# map to reordered labels\n",
    "row_cluster_labels = pd.Series(row_clusters, index=heatrmap_df.index).loc[reordered_rows]\n",
    "col_cluster_labels = pd.Series(col_clusters, index=heatrmap_df.columns).loc[reordered_cols]\n",
    "\n",
    "ax = g.ax_heatmap\n",
    "\n",
    "# find start and length of each cluster in order\n",
    "def get_cluster_spans(cluster_labels):\n",
    "    spans = []\n",
    "    prev_label = None\n",
    "    start_idx = 0\n",
    "    for idx, label in enumerate(cluster_labels):\n",
    "        if label != prev_label and prev_label is not None:\n",
    "            spans.append((prev_label, start_idx, idx))\n",
    "            start_idx = idx\n",
    "        prev_label = label\n",
    "    spans.append((prev_label, start_idx, len(cluster_labels)))\n",
    "    return spans\n",
    "\n",
    "# row cluster spans (y-axis)\n",
    "row_spans = get_cluster_spans(row_cluster_labels.tolist())\n",
    "for label, start, end in row_spans:\n",
    "    height = end - start\n",
    "    rect = patches.Rectangle(\n",
    "        xy=(-0.5, start - 0.5),\n",
    "        width=len(col_cluster_labels),\n",
    "        height=height,\n",
    "        fill=False,\n",
    "        edgecolor='blue',\n",
    "        linewidth=1\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(\n",
    "        x=-1.5,\n",
    "        y=start + height / 2 - 0.5,\n",
    "        s=f\"RCC{label}\",\n",
    "        va='center',\n",
    "        ha='right',\n",
    "        fontsize=15,\n",
    "        color='blue'\n",
    "    )\n",
    "\n",
    "# column cluster spans (x-axis)\n",
    "col_spans = get_cluster_spans(col_cluster_labels.tolist())\n",
    "for label, start, end in col_spans:\n",
    "    width = end - start\n",
    "    rect = patches.Rectangle(\n",
    "        xy=(start - 0.5, -0.5),\n",
    "        width=width,\n",
    "        height=len(row_cluster_labels),\n",
    "        fill=False,\n",
    "        edgecolor='green',\n",
    "        linewidth=1\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(\n",
    "        x=start + width / 2 - 0.5,\n",
    "        y=-10,\n",
    "        s=f\"MF{label}\",\n",
    "        ha='center',\n",
    "        va='top',\n",
    "        fontsize=15,\n",
    "        color='green',\n",
    "        rotation=0\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_cluster_labels_df = pd.DataFrame(row_cluster_labels)\n",
    "row_cluster_labels_df['RCC_cluster'] = row_cluster_labels_df.index\n",
    "row_cluster_labels_df = pd.DataFrame(row_cluster_labels).rename(columns={0: \"row_cluster\"})\n",
    "\n",
    "col_cluster_labels_df = pd.DataFrame(col_cluster_labels)\n",
    "col_cluster_labels_df['MF_cluster'] = col_cluster_labels_df.index\n",
    "col_cluster_labels_df = pd.DataFrame(col_cluster_labels).rename(columns={0: \"col_cluster\"})\n",
    "\n",
    "jaccard_df_compp = jaccard_df_comp.merge(row_cluster_labels_df, on=\"RCC_cluster\")\n",
    "jaccard_df_compp = jaccard_df_compp.merge(col_cluster_labels_df, on=\"MF_cluster\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heatmap = (\n",
    "    jaccard_df_compp\n",
    "    .groupby([\"row_cluster\", \"col_cluster\"])[\"weighted_jaccard\"]\n",
    "    .median()\n",
    "    .reset_index()\n",
    "    .pivot(index=\"row_cluster\", columns=\"col_cluster\", values=\"weighted_jaccard\")\n",
    ")\n",
    "df_heatmap = 1- df_heatmap\n",
    "df_heatmap = df_heatmap.where(df_heatmap <0.90, 0)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "ax = sns.heatmap(df_heatmap, annot=True, cmap=\"viridis\")\n",
    "\n",
    "ax.set_xlabel(\"MF Clusters\")\n",
    "ax.set_ylabel(\"RCC CLusters\")\n",
    "\n",
    "# custom tick labels\n",
    "#ax.set_xticklabels([\"label1\", \"label2\", \"label3\"], rotation=45, ha=\"right\")\n",
    "#ax.set_yticklabels([\"row1\", \"row2\", \"row3\"], rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()\n",
    "\n",
    "for i in jaccard_df_compp[\"row_cluster\"].sort_values().unique():\n",
    "    for j in jaccard_df_compp[\"col_cluster\"].sort_values().unique():\n",
    "        new_df_tmp = jaccard_df_compp[(jaccard_df_compp[\"row_cluster\"] == i) & (jaccard_df_compp[\"col_cluster\"] == j)] \n",
    "        new_df_tmp = new_df_tmp.sort_values(by=\"weighted_jaccard\", ascending=False).head(n=15)\n",
    "        new_df = pd.concat([new_df, new_df_tmp])\n",
    "\n",
    "\n",
    "for i, row in new_df.iterrows():\n",
    "    value = df_heatmap.loc[row[\"row_cluster\"], row[\"col_cluster\"]]\n",
    "    new_df.at[i, \"median_jaccard\"] = value\n",
    "\n",
    "filtered  = new_df[(new_df[\"median_jaccard\"] != 0)]\n",
    "\n",
    "new_df.to_csv(\"all_jaccard_cellpairs_min_5_lr_interactions_all.csv\", index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_cluster_labels_df = pd.DataFrame(row_cluster_labels)\n",
    "row_cluster_labels_df = pd.DataFrame(row_cluster_labels).rename(columns={0: \"row_cluster\"})\n",
    "row_cluster_labels_df['RCC_cluster'] = row_cluster_labels_df.index\n",
    "\n",
    "\n",
    "col_cluster_labels_df = pd.DataFrame(col_cluster_labels)\n",
    "col_cluster_labels_df = pd.DataFrame(col_cluster_labels).rename(columns={0: \"col_cluster\"})\n",
    "col_cluster_labels_df['MF_cluster'] = col_cluster_labels_df.index\n",
    "\n",
    "\n",
    "row_cluster_labels_df[[\"RCC_Sender\", \"RCC_Receiver\"]] = row_cluster_labels_df[\"RCC_cluster\"].str.split(\"@\", expand= True)\n",
    "r_sender_counts = row_cluster_labels_df.groupby([\"row_cluster\", \"RCC_Sender\"]).size().unstack(fill_value=0)\n",
    "r_receiver_counts = row_cluster_labels_df.groupby([\"row_cluster\", \"RCC_Receiver\"]).size().unstack(fill_value=0)\n",
    "r_sender_counts2 = r_sender_counts.add_suffix(\"_as_sender\")\n",
    "r_receiver_counts2 = r_receiver_counts.add_suffix(\"_as_receiver\")\n",
    "row_combined_counts = pd.concat([r_sender_counts2, r_receiver_counts2], axis=1).fillna(0).astype(int)\n",
    "row_combined_counts = row_combined_counts.T\n",
    "\n",
    "col_cluster_labels_df[[\"MF_Sender\", \"MF_Receiver\"]] = col_cluster_labels_df[\"MF_cluster\"].str.split(\"@\", expand= True)\n",
    "c_sender_counts = col_cluster_labels_df.groupby([\"col_cluster\", \"MF_Sender\"]).size().unstack(fill_value=0)\n",
    "c_receiver_counts = col_cluster_labels_df.groupby([\"col_cluster\", \"MF_Receiver\"]).size().unstack(fill_value=0)\n",
    "c_sender_counts2 = c_sender_counts.add_suffix(\"_as_sender\")\n",
    "c_receiver_counts2 = c_receiver_counts.add_suffix(\"_as_receiver\")\n",
    "col_combined_counts = pd.concat([c_sender_counts2, c_receiver_counts2], axis=1).fillna(0).astype(int)\n",
    "col_combined_counts = col_combined_counts.T\n",
    "\n",
    "c_sender_counts = c_sender_counts.T\n",
    "c_receiver_counts = c_receiver_counts.T\n",
    "\n",
    "r_sender_counts = r_sender_counts.T\n",
    "r_receiver_counts = r_receiver_counts.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "\n",
    "for i in row_combined_counts.columns:\n",
    "    for j in col_combined_counts.columns:\n",
    "\n",
    "        value = df_heatmap.loc[i, j]\n",
    "        if value == 0:\n",
    "                continue\n",
    "        \n",
    "        r_s = pd.DataFrame()\n",
    "        r_r = pd.DataFrame()\n",
    "        c_s = pd.DataFrame()\n",
    "        c_r = pd.DataFrame()\n",
    "\n",
    "        r_s[\"counts\"] = r_sender_counts[i]\n",
    "        r_s[\"percentage\"] = (r_s[\"counts\"]/(r_s[\"counts\"].sum()))*100\n",
    "\n",
    "        r_r[\"counts\"] = r_receiver_counts[i]\n",
    "        r_r[\"percentage\"] = (r_r[\"counts\"]/(r_r[\"counts\"].sum()))*100\n",
    "\n",
    "        c_s[\"counts\"] = c_sender_counts[j]\n",
    "        c_s[\"percentage\"] = (c_s[\"counts\"]/(c_s[\"counts\"].sum()))*100\n",
    "\n",
    "        c_r[\"counts\"] = c_receiver_counts[j]\n",
    "        c_r[\"percentage\"] = (c_r[\"counts\"]/(c_r[\"counts\"].sum()))*100\n",
    "\n",
    "\n",
    "        sorted_r_s = r_s.sort_values(by=\"counts\", ascending=False).head(n=5)\n",
    "        sorted_c_s = c_s.sort_values(by=\"counts\", ascending=False).head(n=5)\n",
    "        sorted_r_r = r_r.sort_values(by=\"counts\", ascending=False).head(n=5)\n",
    "        sorted_c_r = c_r.sort_values(by=\"counts\", ascending=False).head(n=5)\n",
    "\n",
    "        sorted_r_s = sorted_r_s[sorted_r_s[\"counts\"]!=0]\n",
    "        sorted_c_s = sorted_c_s[sorted_c_s[\"counts\"]!=0]\n",
    "        sorted_r_r = sorted_r_r[sorted_r_r[\"counts\"]!=0]\n",
    "        sorted_c_r = sorted_c_r[sorted_c_r[\"counts\"]!=0]\n",
    "\n",
    "        lst.append(f\"Cluster Similarity {i} and {j}: {df_heatmap.loc[i, j]} \\n Total number of cellpairs: {r_s[\"counts\"].sum()} (RCC); {c_s[\"counts\"].sum()} (MF) \\n Unique Senders: {len(r_s.index[r_s[\"counts\"] != 0].unique())} (RCC), {len(c_s.index[c_s[\"counts\"] != 0].unique())} (MF) \\n Unique Receivers: {len(r_r.index[r_r[\"counts\"] != 0].unique())} (RCC), {len(c_r.index[c_r[\"counts\"] != 0].unique())} (MF)\")\n",
    "        lst.append(sorted_r_s)\n",
    "        lst.append(sorted_c_s)\n",
    "        lst.append(sorted_r_r)\n",
    "        lst.append(sorted_c_r)\n",
    "        \n",
    "\n",
    "lst\n",
    "with open(\"top5_celltypes_per_cluster_dataset_comparison_normalized_LR_scores.txt\", 'w') as f:\n",
    "    for s in lst:\n",
    "        f.write(str(s) + '\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_receiver_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_combined_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_combined_counts = col_combined_counts.T\n",
    "col_combined_counts\n",
    "#col_combined_counts.to_csv(\"col_combined_counts_thresh_1_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Violinplot of median Jaccards per cluster combination\n",
    "\n",
    "sns.violinplot(jaccard_df_annot.groupby(\n",
    "    [\"row_cluster\", \"col_cluster\"]\n",
    ")[\"weighted_jaccard\"].median(), inner=\"quart\")\n",
    "\n",
    "np.quantile(jaccard_df_annot.groupby(\n",
    "    [\"row_cluster\", \"col_cluster\"]\n",
    ")[\"weighted_jaccard\"].median(), q = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_df_annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()\n",
    "\n",
    "for i in jaccard_df_annot[\"row_cluster\"].sort_values().unique():\n",
    "    for j in jaccard_df_annot[\"col_cluster\"].sort_values().unique():\n",
    "        new_df_tmp = jaccard_df_annot[(jaccard_df_annot[\"row_cluster\"] == i) & (jaccard_df_annot[\"col_cluster\"] == j)] \n",
    "        new_df_tmp = new_df_tmp.sort_values(by=\"weighted_jaccard\", ascending=False).head(n=5)\n",
    "        new_df = pd.concat([new_df, new_df_tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered = new_df[new_df[\"weighted_jaccard\"] > 0.2]\n",
    "cluster_pair_summary\n",
    "\n",
    "for i, row in new_df.iterrows():\n",
    "    value = cluster_pair_summary.loc[row[\"row_cluster\"], row[\"col_cluster\"]]\n",
    "    new_df.at[i, \"median_jaccard\"] = value\n",
    "\n",
    "filtered  = new_df[(new_df[\"median_jaccard\"] != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.to_csv(\"top_jaccard_cellpairs_thresh_1_2_filtered_above_0_07.csv\", index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "correlations = heatrmap_df.corr()\n",
    "correlations_array = np.asarray(heatrmap_df.corr())\n",
    "\n",
    "row_linkage = hierarchy.linkage(\n",
    "    distance.pdist(correlations_array), method='average')\n",
    "\n",
    "col_linkage = hierarchy.linkage(\n",
    "    distance.pdist(correlations_array.T), method='average')\n",
    "\n",
    "sns.clustermap(correlations, row_linkage=row_linkage, col_linkage=col_linkage, method=\"average\",\n",
    "               figsize=(13, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#igraph\n",
    "\n",
    "import igraph as ig\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#jaccard_df_tmp = jaccard_df_quant[jaccard_df_quant[\"weighted_jaccard\"] > 0.1]\n",
    "\n",
    "#mat1 = jaccard_df_tmp.pivot(\n",
    "#    index=\"RCC_cluster\", columns=\"MF_cluster\", values=\"weighted_jaccard\"\n",
    "#)\n",
    "#mat2 = jaccard_df_tmp.pivot(\n",
    "#    index=\"MF_cluster\", columns=\"RCC_cluster\", values=\"weighted_jaccard\"\n",
    "#)\n",
    "\n",
    "#mat3 = pd.concat([mat1, mat2])\n",
    "#mat3 = mat3.sort_index(axis=0).sort_index(axis=1)\n",
    "\n",
    "#mat3 = 1 - mat3\n",
    "#mat3 = mat3.fillna(0)\n",
    "\n",
    "\n",
    "g = ig.Graph.Weighted_Adjacency(\n",
    "    heatrmap_df.to_numpy().tolist(),  # convert to nested list\n",
    "    mode= \"undirected\",      # or ig.ADJ_DIRECTED if applicable\n",
    "    attr=\"weight\"\n",
    ")\n",
    "\n",
    "#node names\n",
    "#g.vs[\"name\"] = list(mat3.index)\n",
    "#g.vs[\"label\"] = g.vs[\"name\"]\n",
    "#g.vs[\"label_size\"] = 3\n",
    "\n",
    "# louvain clustering\n",
    "#clusters = g.community_multilevel(weights=g.es[\"weight\"], return_levels = False)\n",
    "#clusters_list = g.community_multilevel(weights=g.es[\"weight\"], return_levels = True)\n",
    "clusters = g.community_leiden(weights=g.es[\"weight\"], resolution= 0.5, n_iterations=2)\n",
    "\n",
    "#g.vs[\"color\"] = [clusters.membership[i] for i in range(len(g.vs))]\n",
    "\n",
    "#layout = g.layout(\"fr\") \n",
    "\n",
    "ig.plot(clusters, vertex_size=4, edge_width=0.5, target=\"clustered_igraph_quant.png\", \n",
    "        bbox = (500,500), margin = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RCC = RCC[RCC[\"lr_means_normed\"] > 0.10]\n",
    "#MF = MF[MF[\"lr_means_normed\"] > 0.10]\n",
    "\n",
    "mean_LR_RCC = RCC.groupby([\"source\", \"target\"])[\"lr_means_normed\"].mean()\n",
    "mean_LR_RCC.add_prefix(\"RCC_\", axis=0)\n",
    "mean_LR_MF = MF.groupby([\"source\", \"target\"])[\"lr_means_normed\"].mean()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.violinplot(mean_LR_RCC, inner=\"quart\", color=  \"#FF944C\")\n",
    "print(np.quantile(mean_LR_RCC, 0.75))\n",
    "print(np.quantile(mean_LR_MF, 0.75))\n",
    "sns.violinplot(mean_LR_MF, inner=\"quart\", color = \"#94FF8F\")\n",
    "ax.set_ylabel(\"LR Means Normed\")\n",
    "\n",
    "\n",
    "mean_LR_RCC = pd.DataFrame(mean_LR_RCC).reset_index()\n",
    "mean_LR_RCC = mean_LR_RCC.pivot(index = \"source\", columns = \"target\", values = \"lr_means_normed\")\n",
    "mean_LR_RCC = mean_LR_RCC.sort_index(axis=0).sort_index(axis=1).add_prefix(\"RCC_\", axis=0).add_prefix(\"RCC_\", axis=1)\n",
    "\n",
    "mean_LR_MF = pd.DataFrame(mean_LR_MF).reset_index()\n",
    "mean_LR_MF = mean_LR_MF.pivot(index = \"source\", columns = \"target\", values = \"lr_means_normed\")\n",
    "mean_LR_MF = mean_LR_MF.sort_index(axis=0).sort_index(axis=1).add_prefix(\"MF_\", axis=0).add_prefix(\"MF_\", axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_LR_RCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sender to sender and receiver to receiver\n",
    "jaccard_df_comp[[\"MF_sender\", \"MF_receiver\"]] = jaccard_df_comp[\"MF_cluster\"].str.split(\"@\", expand=True)\n",
    "jaccard_df_comp[[\"RCC_sender\", \"RCC_receiver\"]] = jaccard_df_comp[\"RCC_cluster\"].str.split(\"@\", expand=True)\n",
    "\n",
    "jaccard_df_comp_split_s = jaccard_df_comp.groupby([\"MF_sender\", \"RCC_sender\"])[\"weighted_jaccard\"].mean()\n",
    "jaccard_df_comp_split_r = jaccard_df_comp.groupby([\"RCC_receiver\", \"MF_receiver\"])[\"weighted_jaccard\"].mean()\n",
    "\n",
    "jaccard_df_comp_split_s = jaccard_df_comp_split_s.reset_index()\n",
    "jaccard_df_comp_split_s = jaccard_df_comp_split_s.pivot(index = \"RCC_sender\", columns = \"MF_sender\", values= \"weighted_jaccard\").add_prefix(\"RCC_\", axis=0).add_prefix(\"MF_\", axis=1)\n",
    "\n",
    "jaccard_df_comp_split_r = jaccard_df_comp_split_r.reset_index()\n",
    "jaccard_df_comp_split_r = jaccard_df_comp_split_r.pivot(index = \"MF_receiver\", columns = \"RCC_receiver\", values= \"weighted_jaccard\").add_prefix(\"MF_\", axis=0).add_prefix(\"RCC_\", axis=1)\n",
    "jaccard_df_comp_split = pd.concat((jaccard_df_comp_split_s, jaccard_df_comp_split_r))\n",
    "\n",
    "print(np.quantile(jaccard_df_comp_split_s, 0.75))\n",
    "print(np.quantile(jaccard_df_comp_split_r, 0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sender and receiver probabilities\n",
    "\n",
    "jaccard_df_comp_split_s_p = jaccard_df_comp_split_s.div(jaccard_df_comp_split_s.max(axis=1), axis=0) \n",
    "jaccard_df_comp_split_r_p = jaccard_df_comp_split_r.div(jaccard_df_comp_split_r.max(axis=1), axis=0) \n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.violinplot(jaccard_df_comp_split_s_p.mean(), inner=\"quart\", color=  \"#FF944C\")\n",
    "sns.violinplot(jaccard_df_comp_split_r_p.mean(), inner=\"quart\", color = \"#94FF8F\")\n",
    "\n",
    "print(np.quantile(jaccard_df_comp_split_s_p, 0.75))\n",
    "print(np.quantile(jaccard_df_comp_split_r_p, 0.75))\n",
    "\n",
    "jaccard_df_comp_split_s_p = jaccard_df_comp_split_s_p[jaccard_df_comp_split_s_p >= 0.7]\n",
    "jaccard_df_comp_split_r_p = jaccard_df_comp_split_r_p[jaccard_df_comp_split_r_p >= 0.62]\n",
    "\n",
    "jaccard_df_comp_split_p = pd.concat((jaccard_df_comp_split_s_p, jaccard_df_comp_split_r_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_df_comp_split_m = jaccard_df_comp_split_p\n",
    "combined_matrices = pd.DataFrame()\n",
    "\n",
    "median_LR_MF = median_LR_MF[median_LR_MF >=0.23958727957045906]\n",
    "median_LR_RCC = median_LR_RCC[median_LR_RCC >=0.2265910763666835]\n",
    "\n",
    "combined_matrices_LR = pd.concat((median_LR_MF, median_LR_RCC))\n",
    "\n",
    "#thresholds\n",
    "#combined_matrices_LR = combined_matrices_LR[combined_matrices_LR >= mean_quants]#0.23]\n",
    "#jaccard_df_comp_split_m = jaccard_df_comp_split_m[jaccard_df_comp_split_m >= 0.65]\n",
    "\n",
    "#setting Jaccard score to 1\n",
    "#jaccard_df_comp_split_m = jaccard_df_comp_split_m.mask(jaccard_df_comp_split_m < 0.05)\n",
    "#jaccard_df_comp_split_m = jaccard_df_comp_split_m.applymap(lambda x: 1 if pd.notna(x) else np.nan)\n",
    "\n",
    "#normalizing Jaccard and LR before merging to be on same scale\n",
    "#min_val = combined_matrices_LR.min()\n",
    "#max_val = combined_matrices_LR.max()\n",
    "#combined_matrices_LR = (combined_matrices_LR - min_val) / (max_val - min_val)\n",
    "#min_val = jaccard_df_comp_split_m.min()\n",
    "#max_val = jaccard_df_comp_split_m.max()\n",
    "#jaccard_df_comp_split_m = (jaccard_df_comp_split_m - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "\n",
    "combined_matrices = combined_matrices_LR.combine_first(jaccard_df_comp_split_m)\n",
    "combined_matrices = combined_matrices.fillna(0)\n",
    "\n",
    "\n",
    "g_comb = ig.Graph.Weighted_Adjacency(\n",
    "    combined_matrices.to_numpy().tolist(), \n",
    "    mode= \"directed\",     \n",
    "    attr=\"weight\"\n",
    ")\n",
    "\n",
    "#node names\n",
    "g_comb.vs[\"name\"] = list(combined_matrices.index)\n",
    "g_comb.vs[\"label\"] = g_comb.vs[\"name\"]\n",
    "g_comb.vs[\"label_size\"] = 10\n",
    "degree = g_comb.degree()\n",
    "\n",
    "clusters_comb = leidenalg.find_partition(g_comb, weights=g_comb.es[\"weight\"], partition_type= leidenalg.CPMVertexPartition, resolution_parameter= 0.27,seed = 33)\n",
    "print(clusters_comb)\n",
    "#g_comb.vs[\"color\"] = [clusters_comb.membership[i] for i in range(len(g_comb.vs))]\n",
    "\n",
    "layout = g_comb.layout(\"fr\") \n",
    "\n",
    "\n",
    "\n",
    "ig.plot(clusters_comb, layout = layout, vertex_size=30, edge_width=[w[\"weight\"] for w in g_comb.es], edge_alpha= 0.4, target=\"clustered_igraph_quant.png\", \n",
    "        bbox = (1500,1500), margin = 100)\n",
    "\n",
    "#n = g_comb.to_networkx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CPM Vertex\n",
    "\n",
    "resolutions = np.linspace(0, 0.4, 100) \n",
    "results_res = []\n",
    "results_k = []\n",
    "results = []\n",
    "\n",
    "runs = 100\n",
    "\n",
    "for r in resolutions:\n",
    "    aris = []\n",
    "    nmi = []\n",
    "    partition = leidenalg.find_partition(\n",
    "        g_comb,\n",
    "        leidenalg.CPMVertexPartition,\n",
    "        weights=g_comb.es[\"weight\"],\n",
    "        resolution_parameter=r,\n",
    "        seed = 33\n",
    "    )\n",
    "    modularity = partition.modularity\n",
    "    results_res.append(r)\n",
    "    results_k.append(len(partition))\n",
    "    results.append((r, len(partition)))\n",
    "\n",
    "#plt.bar(results_res, results_k, align=\"edge\")\n",
    "#plt.xticks(results_res)\n",
    "#plt.yticks(results_k)\n",
    "#plt.xlim(0,0.4)\n",
    "\n",
    "resultsdf = pd.DataFrame(results)\n",
    "ax = sns.lineplot(x = resultsdf[0], y = resultsdf[1])\n",
    "ax.set(xlabel = \"CPMVertex Resolutions\", ylabel = \"k clusters\", yticks = results_k)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RBERVertex\n",
    "resolutions = np.linspace(0.5, 3, 300) \n",
    "results_res = []\n",
    "results_k = []\n",
    "results = []\n",
    "\n",
    "runs = 100\n",
    "\n",
    "for r in resolutions:\n",
    "    aris = []\n",
    "    nmi = []\n",
    "    partition = leidenalg.find_partition(\n",
    "        g_comb,\n",
    "        leidenalg.RBERVertexPartition,\n",
    "        weights=g_comb.es[\"weight\"],\n",
    "        resolution_parameter=r,\n",
    "        seed = 33\n",
    "    )\n",
    "    modularity = partition.modularity\n",
    "    results_res.append(r)\n",
    "    results_k.append(len(partition))\n",
    "    results.append((r, len(partition)))\n",
    "\n",
    "\n",
    "#plt.bar(results_res, results_k, align=\"edge\")\n",
    "#plt.xticks(results_res)\n",
    "#plt.yticks(results_k)\n",
    "#plt.xlim(0.5, 3)\n",
    "\n",
    "resultsdf = pd.DataFrame(results)\n",
    "ax = sns.lineplot(x = resultsdf[0], y = resultsdf[1])\n",
    "ax.set(xlabel = \"RBERVertex Resolutions\", ylabel = \"k clusters\", yticks = results_k)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RBERVertex\n",
    "\n",
    "jaccard_df_comp_split_m = jaccard_df_comp_split_p\n",
    "combined_matrices = pd.DataFrame()\n",
    "\n",
    "\n",
    "median_LR_MF = median_LR_MF[median_LR_MF >= 0.2399136639826615]\n",
    "median_LR_RCC = median_LR_RCC[median_LR_RCC >= 0.2265910763666835]\n",
    "\n",
    "combined_matrices_LR = pd.concat((median_LR_MF, median_LR_RCC))\n",
    "\n",
    "#thresholds\n",
    "#combined_matrices_LR = combined_matrices_LR[combined_matrices_LR >= 0.23]\n",
    "#jaccard_df_comp_split_m = jaccard_df_comp_split_m[jaccard_df_comp_split_m >= 0.65]\n",
    "\n",
    "#setting Jaccard score to 1\n",
    "#jaccard_df_comp_split_m = jaccard_df_comp_split_m.mask(jaccard_df_comp_split_m < 0.05)\n",
    "#jaccard_df_comp_split_m = jaccard_df_comp_split_m.applymap(lambda x: 1 if pd.notna(x) else np.nan)\n",
    "\n",
    "#normalizing Jaccard and LR before merging to be on same scale\n",
    "#min_val = combined_matrices_LR.min()\n",
    "#max_val = combined_matrices_LR.max()\n",
    "#combined_matrices_LR = (combined_matrices_LR - min_val) / (max_val - min_val)\n",
    "#min_val = jaccard_df_comp_split_m.min()\n",
    "#max_val = jaccard_df_comp_split_m.max()\n",
    "#jaccard_df_comp_split_m = (jaccard_df_comp_split_m - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "\n",
    "combined_matrices = combined_matrices_LR.combine_first(jaccard_df_comp_split_m)\n",
    "combined_matrices = combined_matrices.fillna(0)\n",
    "\n",
    "\n",
    "g_comb = ig.Graph.Weighted_Adjacency(\n",
    "    combined_matrices.to_numpy().tolist(), \n",
    "    mode= \"directed\",     \n",
    "    attr=\"weight\"\n",
    ")\n",
    "\n",
    "#node names\n",
    "g_comb.vs[\"name\"] = list(combined_matrices.index)\n",
    "g_comb.vs[\"label\"] = g_comb.vs[\"name\"]\n",
    "g_comb.vs[\"label_size\"] = 10\n",
    "degree = g_comb.degree()\n",
    "\n",
    "clusters_comb = leidenalg.find_partition(g_comb, weights=g_comb.es[\"weight\"], partition_type= leidenalg.RBERVertexPartition, resolution_parameter= 1.15, seed = 33)\n",
    "print(clusters_comb)\n",
    "#g_comb.vs[\"color\"] = [clusters_comb.membership[i] for i in range(len(g_comb.vs))]\n",
    "\n",
    "layout = g_comb.layout(\"fr\") \n",
    "\n",
    "\n",
    "\n",
    "ig.plot(clusters_comb, layout = layout, vertex_size=30, edge_width=[w[\"weight\"] for w in g_comb.es], edge_alpha= 0.4, target=\"clustered_igraph_quant.png\", \n",
    "        bbox = (1500,1500), margin = 100)\n",
    "\n",
    "#n = g_comb.to_networkx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RBConfigurationVertexPartition\n",
    "resolutions = np.linspace(0.5, 3, 300) \n",
    "results = []\n",
    "results_res = []\n",
    "results_k = []\n",
    "\n",
    "\n",
    "runs = 100\n",
    "\n",
    "for r in resolutions:\n",
    "    aris = []\n",
    "    nmi = []\n",
    "    partition = leidenalg.find_partition(\n",
    "        g_comb,\n",
    "        leidenalg.RBConfigurationVertexPartition,\n",
    "        weights=g_comb.es[\"weight\"],\n",
    "        resolution_parameter=r,\n",
    "        seed = 33\n",
    "    )\n",
    "    modularity = partition.modularity\n",
    "    results_res.append(r)\n",
    "    results_k.append(len(partition))\n",
    "    results.append((r, len(partition)))\n",
    "\n",
    "\n",
    "\n",
    "#plt.bar(results_res, results_k, align=\"edge\", color =\"green\")\n",
    "#plt.xticks(results_res)\n",
    "#plt.xticks(round(results_res[::35], 1))\n",
    "\n",
    "#plt.yticks(results_k)\n",
    "#plt.xlabel(\"Resolutions RBConfiguration\")\n",
    "#plt.ylabel(\"k clusters\")\n",
    "#plt.xlim(0.5, 3)\n",
    "\n",
    "resultsdf = pd.DataFrame(results)\n",
    "ax = sns.lineplot(x = resultsdf[0], y = resultsdf[1])\n",
    "ax.set(xlabel = \"RBConfiguration Resolutions\", ylabel = \"k clusters\", yticks = results_k)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RBConfigurationVertexPartition\n",
    "\n",
    "jaccard_df_comp_split_m = jaccard_df_comp_split_p\n",
    "combined_matrices = pd.DataFrame()\n",
    "\n",
    "\n",
    "mean_LR_MF = mean_LR_MF[mean_LR_MF >= 0.2399136639826615]\n",
    "mean_LR_RCC = mean_LR_RCC[mean_LR_RCC >= 0.2265910763666835]\n",
    "\n",
    "combined_matrices_LR = pd.concat((mean_LR_MF, mean_LR_RCC))\n",
    "\n",
    "#thresholds\n",
    "#combined_matrices_LR = combined_matrices_LR[combined_matrices_LR >= 0.23]\n",
    "#jaccard_df_comp_split_m = jaccard_df_comp_split_m[jaccard_df_comp_split_m >= 0.65]\n",
    "\n",
    "#setting Jaccard score to 1\n",
    "#jaccard_df_comp_split_m = jaccard_df_comp_split_m.mask(jaccard_df_comp_split_m < 0.05)\n",
    "#jaccard_df_comp_split_m = jaccard_df_comp_split_m.applymap(lambda x: 1 if pd.notna(x) else np.nan)\n",
    "\n",
    "#normalizing Jaccard and LR before merging to be on same scale\n",
    "#min_val = combined_matrices_LR.min()\n",
    "#max_val = combined_matrices_LR.max()\n",
    "#combined_matrices_LR = (combined_matrices_LR - min_val) / (max_val - min_val)\n",
    "#min_val = jaccard_df_comp_split_m.min()\n",
    "#max_val = jaccard_df_comp_split_m.max()\n",
    "#jaccard_df_comp_split_m = (jaccard_df_comp_split_m - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "\n",
    "combined_matrices = combined_matrices_LR.combine_first(jaccard_df_comp_split_m)\n",
    "combined_matrices = combined_matrices.fillna(0)\n",
    "\n",
    "\n",
    "g_comb = ig.Graph.Weighted_Adjacency(\n",
    "    combined_matrices.to_numpy().tolist(), \n",
    "    mode= \"directed\",     \n",
    "    attr=\"weight\"\n",
    ")\n",
    "\n",
    "#node names\n",
    "g_comb.vs[\"name\"] = list(combined_matrices.index)\n",
    "g_comb.vs[\"label\"] = g_comb.vs[\"name\"]\n",
    "g_comb.vs[\"label_size\"] = 10\n",
    "degree = g_comb.degree()\n",
    "\n",
    "clusters_comb = leidenalg.find_partition(g_comb, weights=g_comb.es[\"weight\"], partition_type= leidenalg.RBConfigurationVertexPartition, resolution_parameter= 1.25, seed = 33)\n",
    "print(clusters_comb)\n",
    "#g_comb.vs[\"color\"] = [clusters_comb.membership[i] for i in range(len(g_comb.vs))]\n",
    "\n",
    "layout = g_comb.layout(\"fr\") \n",
    "\n",
    "\n",
    "\n",
    "ig.plot(clusters_comb, layout = layout, vertex_size=30, edge_width=[w[\"weight\"] for w in g_comb.es], edge_alpha= 0.4, target=\"clustered_igraph_quant.png\", \n",
    "        bbox = (1500,1500), margin = 100)\n",
    "\n",
    "#n = g_comb.to_networkx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# --- extract membership ---\n",
    "labels = clusters_comb.membership\n",
    "\n",
    "# --- convert weights to distances ---\n",
    "distances = []\n",
    "for e in g_comb.es:\n",
    "    w = e[\"weight\"]\n",
    "    dist = 1.0 / w if w > 0 else 1e9\n",
    "    distances.append(dist)\n",
    "g_comb.es[\"dist\"] = distances\n",
    "\n",
    "# --- compute shortest-path distance matrix ---\n",
    "D = np.array(g_comb.distances(weights=\"dist\"))\n",
    "\n",
    "# --- silhouette computation ---\n",
    "sil = silhouette_score(D, labels, metric=\"precomputed\")\n",
    "print(\"Silhouette score:\", sil)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1  extract cluster labels\n",
    "labels = clusters_comb.membership\n",
    "\n",
    "# 2  convert edge weights to distances\n",
    "distances = []\n",
    "for e in g_comb.es:\n",
    "    w = e[\"weight\"]\n",
    "    dist = 1.0 / w if w > 0 else 1e9\n",
    "    distances.append(dist)\n",
    "g_comb.es[\"dist\"] = distances\n",
    "\n",
    "# 3  graph shortest-path distances\n",
    "D = np.array(g_comb.shortest_paths(weights=\"dist\"))\n",
    "\n",
    "# 4  silhouette per node\n",
    "sil_samples = silhouette_samples(D, labels, metric=\"precomputed\")\n",
    "\n",
    "# 5  dataframe\n",
    "sil_df = pd.DataFrame({\n",
    "    \"node\": g_comb.vs[\"name\"],\n",
    "    \"cluster\": labels,\n",
    "    \"silhouette\": sil_samples\n",
    "})\n",
    "\n",
    "# 6  mean silhouette per cluster\n",
    "cluster_silhouette = sil_df.groupby(\"cluster\")[\"silhouette\"].mean().sort_values(ascending=False)\n",
    "print(cluster_silhouette)\n",
    "\n",
    "# 7  plot\n",
    "plt.figure(figsize=(9,5))\n",
    "sns.boxplot(data=sil_df, x=\"cluster\", y=\"silhouette\")\n",
    "sns.stripplot(data=sil_df, x=\"cluster\", y=\"silhouette\", color=\"black\", alpha=0.4, size=2)\n",
    "plt.axhline(0, color=\"gray\", linestyle=\"--\")\n",
    "plt.ylabel(\"Silhouette value\")\n",
    "plt.title(\"Silhouette Score per Leiden cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RBConfig res 1.25 after removing glom endo\n",
    "cluster_dict = {\"Monocytes\" : [\"MF_CD16_monocytes\", \"MF_Classical_monocytes\", \"MF_EBM_prog\", \"MF_GMPs\",\n",
    "    \"MF_Inflammatory_CD14_monocytes\", \"MF_Late_Neutro_Prog\", \"MF_Macrophages\",\n",
    "    \"MF_Neutrophils\", \"MF_SPP1_Macrophages\", \"MF_cDC1\", \"MF_cDC2\", \"RCC_Classical monocytes\", \"RCC_Non-classical monocytes\", \"RCC_TAM 1\", \"RCC_TAM 2\", \"RCC_TAM 3\", \"RCC_TAM 4\"],\n",
    "\"Lymphocytes\" : [\"MF_B_cells\", \"MF_CD8_T_cells\",\"MF_NK_T_cells\", \"RCC_B cells\",\n",
    "    \"RCC_CD8 T cells\", \"RCC_Cytotoxic T cells\", \"RCC_IGHG-high plasma cells\",\n",
    "    \"RCC_NK cells\", \"RCC_Plasma cells\", \"RCC_Regulatory T cells\",\n",
    "    \"RCC_Resting/memory T cells\"],#, \"RCC_Proximal tubule\"],\n",
    "\"Tumor_prog\" : [\"MF_B_cell_prog\", \"MF_Plasma_cells\",\n",
    "                 \"MF_Early_Neutro_Prog\", \"MF_HSPCs\", \"MF_MEPs\", \"MF_MK_prog\",\n",
    "    \"RCC_DCT/CNT\", \"RCC_Epithelial progenitor-like cells\", \"RCC_Principal cells\",\n",
    "    \"RCC_Tumor cells 1\", \"RCC_Tumor cells 2\", \"RCC_Tumor cells 3\", \"RCC_tAL of LOH\"],\n",
    "\"Endothelial\" : [ \"MF_pDCs\", \"MF_Arterial_EC\", \"MF_Erythroid_cells\", \"MF_Late_Erythroid_Prog\",\n",
    "    \"MF_Sinusoidal_EC\", \"RCC_AVR\", \"RCC_DVR\", \"RCC_Tumor AVR-like vasculature\",\n",
    "    \"RCC_Tumor vasculature 1\", \"RCC_Tumor vasculature 2\", \"RCC_Tumor vasculature 3\", \"RCC_Tumor vasculature 4\"],\n",
    "\"Stromal\" : [\"MF_Adipo_CAR\", \"MF_Mature_MKs\", \"MF_OLCs\", \"MF_Osteoblasts\", \"RCC_Mesangial/vSMCs\",\n",
    "    \"RCC_Myofibroblasts\", \"RCC_vSMCs\"],\n",
    "\"Mast_cells\" : [\"MF_Mast_cells\", \"RCC_Mast cells\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraphs = [g_comb.subgraph(cluster) for cluster in clusters_comb]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(subgraphs), figsize=(10*len(subgraphs), 10))\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import colors as mcolors\n",
    "\n",
    "cluster_names = list(cluster_dict.keys())\n",
    "cluster_names = [\"Myeloids\", \"Tumor/prog\", \"Endothelial\", \"Lymphocytes\", \"Stromal\", \"Mast cells\"]\n",
    "\n",
    "# pick a colormap\n",
    "cmap = cm.get_cmap(\"tab10\", len(subgraphs))  # up to 10 distinct colors\n",
    "\n",
    "colors = [cmap(i) for i in range(len(subgraphs))]\n",
    "edge_alpha = 0.4  # edge transparency\n",
    "edge_color = mcolors.to_rgba(\"black\", alpha=edge_alpha)\n",
    "\n",
    "\n",
    "for i, (sg, cluster_name) in enumerate(zip(subgraphs, cluster_names)):\n",
    "    filename = f\"{cluster_name.replace('/', '_').replace(' ', '_')}_igraph.png\"\n",
    "    layout = sg.layout(\"fr\")  # Fruchterman-Reingold (force-directed)\n",
    "    color = colors[i]\n",
    "    ig.plot(\n",
    "        sg,\n",
    "        #target=axes[i],\n",
    "        layout=layout,\n",
    "        vertex_size=30,\n",
    "        vertex_label=sg.vs[\"name\"],\n",
    "        vertex_label_size = 15,\n",
    "        vertex_label_dist = 2,\n",
    "        target=filename,\n",
    "        edge_width=[w for w in sg.es[\"weight\"]],\n",
    "        vertex_color=[color] * sg.vcount(),\n",
    "        edge_color=[edge_color] * sg.ecount(),\n",
    "        margin=20\n",
    "    )\n",
    "    axes[i].set_title(f\"{cluster_name}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#for clust in clusters_comb:\n",
    "#    ig.plot(clust, layout = layout, vertex_size=30, edge_width=[w[\"weight\"] for w in g_comb.es], edge_alpha= 0.4, target=\"clustered_igraph_quant.png\", \n",
    "#        bbox = (1500,1500), margin = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "from matplotlib import colors as mcolors\n",
    "import igraph as ig\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# assuming: subgraphs = [g_comb.subgraph(cluster) for cluster in clusters_comb]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(subgraphs), figsize=(12*len(subgraphs), 12))\n",
    "\n",
    "cluster_names = [\"Myeloids\", \"Tumor/prog\", \"Endothelial\", \"Lymphocytes\", \"Stromal\", \"Mast cells\"]\n",
    "\n",
    "# base colormap for subgraph backgrounds\n",
    "cmap = cm.get_cmap(\"tab10\", len(subgraphs))\n",
    "colors = [cmap(i) for i in range(len(subgraphs))]\n",
    "\n",
    "# edge transparency and color\n",
    "edge_alpha = 0.4\n",
    "edge_color = mcolors.to_rgba(\"black\", alpha=edge_alpha)\n",
    "\n",
    "def vertex_color_by_prefix(name):\n",
    "    if name.startswith(\"RCC\"):\n",
    "        return \"#FF944C\"  \n",
    "    elif name.startswith(\"MF\"):\n",
    "        return  \"#7FCE7B\"\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor=\"#FF944C\", edgecolor=\"black\", label=\"RCC\"),\n",
    "    Patch(facecolor=\"#7FCE7B\", edgecolor=\"black\", label=\"MF\")\n",
    "]\n",
    "\n",
    "for i, (sg, cluster_name) in enumerate(zip(subgraphs, cluster_names)):\n",
    "    layout = sg.layout(\"fr\")  # Fruchterman-Reingold (force-directed)\n",
    "\n",
    "    # generate per-vertex colors based on prefix\n",
    "    vertex_colors = [vertex_color_by_prefix(v[\"name\"]) for v in sg.vs]\n",
    "    filename = f\"{cluster_name.replace('/', '_').replace(' ', '_')}_igraph.png\"\n",
    "    ig.plot(\n",
    "        sg,\n",
    "        target=filename,\n",
    "        layout=layout,\n",
    "        vertex_size=20,\n",
    "        vertex_label=sg.vs[\"name\"],\n",
    "        vertex_label_size=15,\n",
    "        vertex_label_dist=2,\n",
    "        edge_width=sg.es[\"weight\"],\n",
    "        vertex_color=vertex_colors,\n",
    "        edge_color=[edge_color] * sg.ecount(),\n",
    "        margin = 60\n",
    "    )\n",
    "\n",
    "    # --- 2. open that image with matplotlib and add legend ---\n",
    "    img = plt.imread(filename)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # add legend inside the image (bottom-right corner)\n",
    "    ax.legend(\n",
    "        handles=legend_elements,\n",
    "        loc=\"lower right\",\n",
    "        frameon=False,\n",
    "        fontsize=10,\n",
    "        ncol=1\n",
    "    )\n",
    "\n",
    "    # save updated image with legend\n",
    "    outname = f\"{cluster_name.replace('/', '_').replace(' ', '_')}_igraph_with_legend.png\"\n",
    "    plt.savefig(outname, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved: {outname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "from matplotlib import colors as mcolors\n",
    "import igraph as ig\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# assuming: subgraphs = [g_comb.subgraph(cluster) for cluster in clusters_comb]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(subgraphs), figsize=(12*len(subgraphs), 12))\n",
    "\n",
    "cluster_names = [\"Myeloids\", \"Tumor/prog\", \"Endothelial\", \"Lymphocytes\", \"Stromal\", \"Mast cells\"]\n",
    "\n",
    "# base colormap for subgraph backgrounds\n",
    "cmap = cm.get_cmap(\"tab10\", len(subgraphs))\n",
    "colors = [cmap(i) for i in range(len(subgraphs))]\n",
    "\n",
    "# edge transparency and color\n",
    "edge_alpha = 0.4\n",
    "edge_color = mcolors.to_rgba(\"black\", alpha=edge_alpha)\n",
    "\n",
    "# prefix-based color assignment\n",
    "def vertex_color_by_prefix(name):\n",
    "    if name.startswith(\"RCC\"):\n",
    "        return \"#FF944C\"  \n",
    "    elif name.startswith(\"MF\"):\n",
    "        return \"#7FCE7B\"\n",
    "    else:\n",
    "        return \"#CCCCCC\"  # default color if neither prefix matches\n",
    "\n",
    "# define legend elements ONCE\n",
    "legend_elements = [\n",
    "    Patch(facecolor=\"#FF944C\", edgecolor=\"black\", label=\"RCC\"),\n",
    "    Patch(facecolor=\"#7FCE7B\", edgecolor=\"black\", label=\"MF\")\n",
    "]\n",
    "\n",
    "for i, (sg, cluster_name) in enumerate(zip(subgraphs, cluster_names)):\n",
    "    layout = sg.layout(\"fr\")  # Fruchterman-Reingold (force-directed)\n",
    "\n",
    "    # per-vertex colors based on prefix\n",
    "    vertex_colors = [vertex_color_by_prefix(v[\"name\"]) for v in sg.vs]\n",
    "    filename = f\"{cluster_name.replace('/', '_').replace(' ', '_')}_igraph.png\"\n",
    "\n",
    "    # --- 1. Save igraph plot ---\n",
    "    ig.plot(\n",
    "        sg,\n",
    "        target=filename,\n",
    "        layout=layout,\n",
    "        vertex_size=20,\n",
    "        vertex_label=sg.vs[\"name\"],\n",
    "        vertex_label_size=15,\n",
    "        vertex_label_dist=2,\n",
    "        edge_width=sg.es[\"weight\"],\n",
    "        vertex_color=vertex_colors,\n",
    "        edge_color=[edge_color] * sg.ecount(),\n",
    "        margin=80\n",
    "    )\n",
    "\n",
    "    # --- 2. Add legend inside image ---\n",
    "    img = plt.imread(filename)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "    # add legend inside (bottom right corner)\n",
    "    ax.legend(\n",
    "        handles=legend_elements,\n",
    "        loc=\"upper left\",\n",
    "        frameon=False,\n",
    "        fontsize=12,\n",
    "        ncol=1\n",
    "    )\n",
    "\n",
    "    outname = f\"{cluster_name.replace('/', '_').replace(' ', '_')}_igraph_with_legend.png\"\n",
    "    plt.savefig(outname, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved: {outname}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_rgb01(hexcolor):\n",
    "    h = hexcolor.lstrip(\"#\")\n",
    "    return tuple(int(h[i:i+2], 16)/255 for i in (0, 2, 4))\n",
    "\n",
    "legend_elements = [\n",
    "    (\"RCC\", hex_to_rgb01(\"#FF944C\")),\n",
    "    (\"MF\",  hex_to_rgb01(\"#7FCE7B\"))\n",
    "]\n",
    "\n",
    "for sg, cluster_name in zip(subgraphs, cluster_names):\n",
    "    layout = sg.layout(\"fr\")\n",
    "    vertex_colors = [vertex_color_by_prefix(v[\"name\"]) for v in sg.vs]\n",
    "\n",
    "    # convert hex to RGB 0-1 for Cairo\n",
    "    vertex_colors = [hex_to_rgb01(c) for c in vertex_colors]\n",
    "\n",
    "    outname = f\"{cluster_name.replace('/', '_').replace(' ', '_')}_igraph_with_legend.svg\"\n",
    "\n",
    "    draw_svg_with_legend(\n",
    "        sg,\n",
    "        layout,\n",
    "        vertex_colors,\n",
    "        legend_elements,\n",
    "        outname\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "g_comb.vs[\"cluster\"] = clusters_comb.membership\n",
    "k = len(set(clusters_comb.membership))\n",
    "\n",
    "prefix_to_marker = {\n",
    "    \"RCC\": \"o\",  # circle\n",
    "    \"MF\": \"s\"   # square\n",
    "}\n",
    "\n",
    "grid_cols = int(np.ceil(np.sqrt(k)))\n",
    "grid_rows = int(np.ceil(k / grid_cols))\n",
    "spacing = 3.0\n",
    "\n",
    "cluster_positions = [\n",
    "    (i % grid_cols * spacing, i // grid_cols * spacing) for i in range(k)\n",
    "]\n",
    "\n",
    "layout_coords = np.zeros((g_comb.vcount(), 2))\n",
    "for i in range(k):\n",
    "    cluster_nodes = [v.index for v in g_comb.vs if v[\"cluster\"] == i]\n",
    "    subg = g_comb.subgraph(cluster_nodes)\n",
    "    sub_layout = np.array(subg.layout(\"fr\"))\n",
    "    sub_layout = (sub_layout - sub_layout.mean(0)) / (sub_layout.std(0) + 1e-9)\n",
    "    sub_layout = sub_layout * 0.8 + cluster_positions[i]\n",
    "    layout_coords[cluster_nodes, :] = sub_layout\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Node + Edge properties\n",
    "weights = np.array(g_comb.es[\"weight\"])\n",
    "abs_weights = np.abs(weights)\n",
    "clusters = np.array(g_comb.vs[\"cluster\"])\n",
    "cmap_nodes = cm.get_cmap(\"tab10\", k)\n",
    "node_colors = [cmap_nodes(c) for c in clusters]\n",
    "\n",
    "# Edge color gradient: gray  red\n",
    "cmap_edges = cm.get_cmap(\"Greys\")\n",
    "norm = Normalize(vmin=abs_weights.min(), vmax=abs_weights.max())\n",
    "edge_colors = [cmap_edges(norm(w)) for w in abs_weights]\n",
    "edge_widths = [w[\"weight\"] for w in g_comb.es]\n",
    "edges = np.array(g_comb.get_edgelist())\n",
    "\n",
    "# Plot edges\n",
    "for (src, tgt), color, w in zip(edges, edge_colors, edge_widths):\n",
    "    ax.plot(\n",
    "        [layout_coords[src, 0], layout_coords[tgt, 0]],\n",
    "        [layout_coords[src, 1], layout_coords[tgt, 1]],\n",
    "        color=color,\n",
    "        alpha=0.3,\n",
    "        linewidth=w,\n",
    "        zorder=1\n",
    "    )\n",
    "\n",
    "# Node labels\n",
    "#for i, name in enumerate(g_comb.vs[\"name\"]):\n",
    "#    ax.text(\n",
    "#        layout_coords[i, 0],\n",
    "#        layout_coords[i, 1] -0.15,\n",
    "#        name,\n",
    "#        fontsize=9,\n",
    "#        ha=\"center\",\n",
    "#        va=\"center\",\n",
    "#        zorder=3\n",
    "#    )\n",
    "\n",
    "node_labels = g_comb.vs[\"name\"]\n",
    "layout_coords = np.array(layout_coords) \n",
    "# Plot nodes\n",
    "for prefix, marker in prefix_to_marker.items():\n",
    "    idx = [i for i, name in enumerate(node_labels) if name.startswith(prefix)]\n",
    "    ax.scatter(\n",
    "        layout_coords[idx, 0],\n",
    "        layout_coords[idx, 1],\n",
    "        s=100,\n",
    "        c=[cmap_nodes(clusters[i]) for i in idx],\n",
    "        edgecolor=\"black\",\n",
    "        marker=marker,\n",
    "        zorder=2\n",
    "    )\n",
    "\n",
    "\n",
    "legend_handles = [mpatches.Patch(color=cmap(i), label=f\"{j}\") for i, j in enumerate(cluster_dict.keys())]\n",
    "ax.legend(\n",
    "    handles=legend_handles,\n",
    "    title=\"Leiden Clusters\",\n",
    "    bbox_to_anchor=(1.05, 1),\n",
    "    loc=\"upper left\",\n",
    "    borderaxespad=0.0,\n",
    "    fontsize=8,\n",
    "    title_fontsize=9\n",
    ")\n",
    "\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap_edges, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=ax, fraction=0.01, pad=0.07)\n",
    "cbar.set_label(\"Edge weight [%]\", fontsize=8)\n",
    "\n",
    "\n",
    "dataset_handles = [\n",
    "    Line2D([0], [0], marker='o', color='w', label='RCC', markerfacecolor='gray', markersize=10),\n",
    "    Line2D([0], [0], marker='s', color='w', label='MF', markerfacecolor='gray', markersize=10)\n",
    "]\n",
    "\n",
    "ax.legend(handles=legend_handles + dataset_handles, title=\"Legend\", fontsize=8, title_fontsize=9,\n",
    "          handlelength=1, handleheight=0.5, labelspacing=0.2, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "\n",
    "#ax.set_title(\"Clustered directed graph (Leiden + RBConfiguration)\", fontsize=16)\n",
    "ax.axis(\"off\")\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "fig.savefig(\"network_plot.svg\", format=\"svg\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyvoi bc i had import issues (chatgpt version delivers same results)\n",
    "import numpy as np\n",
    "\n",
    "import numpy.typing as npt\n",
    "import warnings\n",
    "\n",
    "\n",
    "def VI_np(labels1,labels2,return_split_merge=False):\n",
    "    assert len(labels2)==len(labels1)\n",
    "    size=len(labels2)\n",
    "\n",
    "    mutual_labels=(labels1.astype(np.uint64)<<32)+labels2.astype(np.uint64)\n",
    "\n",
    "    sm_unique,sm_inverse,sm_counts=np.unique(labels2,return_inverse=True,return_counts=True)\n",
    "    fm_unique,fm_inverse,fm_counts=np.unique(labels1,return_inverse=True,return_counts=True)\n",
    "    _,mutual_inverse,mutual_counts=np.unique(mutual_labels,return_inverse=True,return_counts=True)\n",
    "\n",
    "    terms_mutual = -np.log(mutual_counts/size)*mutual_counts/size\n",
    "    terms_mutual_per_count=terms_mutual[mutual_inverse]/mutual_counts[mutual_inverse]\n",
    "    terms_sm = -np.log(sm_counts/size)*sm_counts/size\n",
    "    terms_fm = -np.log(fm_counts/size)*fm_counts/size\n",
    "    if not return_split_merge:\n",
    "        terms_mutual_sum=np.sum(terms_mutual_per_count)\n",
    "        vi_split=terms_mutual_sum-terms_sm.sum()\n",
    "        vi_merge=terms_mutual_sum-terms_fm.sum()\n",
    "        vi=vi_split+vi_merge\n",
    "        return vi,vi_split,vi_merge\n",
    "\n",
    "    vi_split_each=np.zeros(len(sm_unique))\n",
    "    np.add.at(vi_split_each,sm_inverse,terms_mutual_per_count)\n",
    "    vi_split_each-=terms_sm\n",
    "    vi_merge_each=np.zeros(len(fm_unique))\n",
    "    np.add.at(vi_merge_each,fm_inverse,terms_mutual_per_count)\n",
    "    vi_merge_each-=terms_fm\n",
    "\n",
    "    vi_split=np.sum(vi_split_each)\n",
    "    vi_merge=np.sum(vi_merge_each)\n",
    "    vi=vi_split+vi_merge\n",
    "\n",
    "    i_splitters=np.argsort(vi_split_each)[::-1]\n",
    "    i_mergers=np.argsort(vi_merge_each)[::-1]\n",
    "\n",
    "    vi_split_sorted=vi_split_each[i_splitters]\n",
    "    vi_merge_sorted=vi_merge_each[i_mergers]\n",
    "\n",
    "    splitters=np.stack([vi_split_sorted,sm_unique[i_splitters]],axis=1)\n",
    "    mergers=np.stack([vi_merge_sorted,fm_unique[i_mergers]],axis=1)\n",
    "    return vi,vi_split,vi_merge,splitters,mergers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolutions = np.linspace(0.5, 1.1, 20) \n",
    "results = []\n",
    "\n",
    "\n",
    "def get_partition_labels(g, resolution, seed):\n",
    "    partition = leidenalg.find_partition(\n",
    "        g_comb,\n",
    "        leidenalg.RBERVertexPartition,\n",
    "        resolution_parameter=resolution,\n",
    "        weights=g_comb.es[\"weight\"],\n",
    "        seed=seed\n",
    "    )\n",
    "    return partition.membership\n",
    "\n",
    "runs = 100\n",
    "\n",
    "for r in resolutions:\n",
    "    aris = []\n",
    "    nmi = []\n",
    "    partition = leidenalg.find_partition(\n",
    "        g_comb,\n",
    "        leidenalg.RBERVertexPartition,\n",
    "        weights=g_comb.es[\"weight\"],\n",
    "        resolution_parameter=r\n",
    "    )\n",
    "    modularity = partition.modularity\n",
    "\n",
    "    labels = [get_partition_labels(g_comb, r, seed=i) for i in range(runs)]\n",
    "\n",
    "    for i in range(runs):\n",
    "        for j in range(i+1, runs):\n",
    "            aris.append(adjusted_rand_score(labels[i], labels[j]))\n",
    "            nmi.append(normalized_mutual_info_score(labels[i], labels[j]))\n",
    "            vi,vi_split,vi_merge=VI_np(np.array(labels[i]), np.array(labels[j]),return_split_merge=False)\n",
    "    results.append((r, np.mean(aris), np.mean(nmi), np.mean(vi), modularity, len(partition)))\n",
    "\n",
    "for r, aris_mean, nmi, vi, mod, k in results:\n",
    "    print(f\"Resolution {r:.2f}: mean ARI = {aris_mean:.3f}; mean NMI = {nmi:.3f}; mean VI = {vi:.3f}; modularity={mod:.3f}; n_clusters={k}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resolutions = np.linspace(0.1, 0.3, 20) \n",
    "results = []\n",
    "\n",
    "\n",
    "def get_partition_labels(g, resolution, seed):\n",
    "    partition = leidenalg.find_partition(\n",
    "        g_comb,\n",
    "        leidenalg.CPMVertexPartition,\n",
    "        resolution_parameter=resolution,\n",
    "        weights=g_comb.es[\"weight\"],\n",
    "        seed=seed+200\n",
    "    )\n",
    "    return partition.membership\n",
    "\n",
    "runs = 100\n",
    "\n",
    "for r in resolutions:\n",
    "    aris = []\n",
    "    nmi = []\n",
    "    partition = leidenalg.find_partition(\n",
    "        g_comb,\n",
    "        leidenalg.CPMVertexPartition,\n",
    "        weights=g_comb.es[\"weight\"],\n",
    "        resolution_parameter=r\n",
    "    )\n",
    "    modularity = partition.modularity\n",
    "\n",
    "    labels = [get_partition_labels(g_comb, r, seed=i) for i in range(runs)]\n",
    "\n",
    "    for i in range(runs):\n",
    "        for j in range(i+1, runs):\n",
    "            aris.append(adjusted_rand_score(labels[i], labels[j]))\n",
    "            nmi.append(normalized_mutual_info_score(labels[i], labels[j]))\n",
    "            vi,vi_split,vi_merge=VI_np(np.array(labels[i]), np.array(labels[j]),return_split_merge=False)\n",
    "    results.append((r, np.mean(aris), np.mean(nmi), np.mean(vi), modularity, len(partition)))\n",
    "\n",
    "for r, aris_mean, nmi, vi, mod, k in results:\n",
    "    print(f\"Resolution {r:.2f}: mean ARI = {aris_mean:.3f}; mean NMI = {nmi:.3f}; mean VI = {vi:.3f}; modularity={mod:.3f}; n_clusters={k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resolutions = np.linspace(0.15, 0.35, 20) \n",
    "results = []\n",
    "\n",
    "\n",
    "def get_partition_labels(g, resolution, seed):\n",
    "    partition = leidenalg.find_partition(\n",
    "        g_comb,\n",
    "        leidenalg.CPMVertexPartition,\n",
    "        resolution_parameter=resolution,\n",
    "        weights=g_comb.es[\"weight\"],\n",
    "        seed=seed+500\n",
    "    )\n",
    "    return partition.membership\n",
    "\n",
    "runs = 100\n",
    "\n",
    "for r in resolutions:\n",
    "    aris = []\n",
    "    nmi = []\n",
    "    partition = leidenalg.find_partition(\n",
    "        g_comb,\n",
    "        leidenalg.CPMVertexPartition,\n",
    "        weights=g_comb.es[\"weight\"],\n",
    "        resolution_parameter=r\n",
    "    )\n",
    "    modularity = partition.modularity\n",
    "\n",
    "    labels = [get_partition_labels(g_comb, r, seed=i) for i in range(runs)]\n",
    "\n",
    "    for i in range(runs):\n",
    "        for j in range(i+1, runs):\n",
    "            aris.append(adjusted_rand_score(labels[i], labels[j]))\n",
    "            nmi.append(normalized_mutual_info_score(labels[i], labels[j]))\n",
    "            vi,vi_split,vi_merge=VI_np(np.array(labels[i]), np.array(labels[j]),return_split_merge=False)\n",
    "    results.append((r, np.mean(aris), np.mean(nmi), np.mean(vi), modularity, len(partition)))\n",
    "\n",
    "for r, aris_mean, nmi, vi, mod, k in results:\n",
    "    print(f\"Resolution {r:.2f}: mean ARI = {aris_mean:.3f}; mean NMI = {nmi:.3f}; mean VI = {vi:.3f}; modularity={mod:.3f}; n_clusters={k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "import random\n",
    "g_name = g_comb\n",
    "\n",
    "def get_partition_labels(g, resolution, seed):\n",
    "    partition = leidenalg.find_partition(\n",
    "        g_name,\n",
    "        leidenalg.CPMVertexPartition,\n",
    "        resolution_parameter=resolution,\n",
    "        weights=g_name.es[\"weight\"],\n",
    "        seed=seed\n",
    "    )\n",
    "    return partition.membership\n",
    "\n",
    "res = 0.4\n",
    "runs = 200\n",
    "labels = [get_partition_labels(g_name, res, seed=i) for i in range(runs)]\n",
    "\n",
    "aris = []\n",
    "for i in range(runs):\n",
    "    for j in range(i+1, runs):\n",
    "        aris.append(adjusted_rand_score(labels[i], labels[j]))\n",
    "\n",
    "print(f\"Resolution {res}: mean ARI = {np.mean(aris):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#louvain\n",
    "igraph_cluster_annot = pd.DataFrame({\"node\": g.vs[\"name\"]})\n",
    "\n",
    "# Map each level's membership back\n",
    "for i, level in enumerate(clusters_list):\n",
    "    igraph_cluster_annot[f\"level_{i}\"] = level.membership\n",
    "\n",
    "igraph_cluster_annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leiden (no return levels)\n",
    "igraph_cluster_annot = pd.DataFrame({\n",
    "    \"node\": g.vs[\"name\"],\n",
    "    \"leiden_cluster\": clusters.membership\n",
    "})\n",
    "\n",
    "igraph_cluster_annot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "\n",
    "clusterable_embedding = umap.UMAP(\n",
    "    n_neighbors=4,\n",
    "    min_dist=0.0,\n",
    "    n_components=2,\n",
    "    random_state=42,\n",
    ").fit_transform(combined_matrices)\n",
    "\n",
    "clusterable_embedding.shape\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "plt.scatter(\n",
    "    clusterable_embedding[:, 0],\n",
    "    clusterable_embedding[:, 1],\n",
    "    s = 3,\n",
    "    )\n",
    "\n",
    "embedding_df = pd.DataFrame(clusterable_embedding, columns=[\"UMAP1\", \"UMAP2\"])\n",
    "embedding_df[\"cellpair\"] = np.array(combined_matrices.index + combined_matrices.columns)\n",
    "\n",
    "for i, row in embedding_df.iterrows():\n",
    "    plt.text(row['UMAP1'], row['UMAP2'], row['cellpair'], fontsize=10)\n",
    "\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP Jaccard Distances', fontsize=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MDS\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "import sklearn.datasets as dt\n",
    "#from sklearn.metrics.pairwise import manhattan_distances, euclidean_distances\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "#jaccard_df[\"jaccard_distance\"] = 1 - jaccard_df[\"weighted_jaccard\"]\n",
    "#jaccard_df_clustermap = jaccard_df.pivot(index=\"RCC_cluster_pair\", columns=\"MF_cluster_pair\", values= \"jaccard_distance\")\n",
    "mds = MDS(dissimilarity=\"precomputed\", random_state=0, max_iter=200, metric=False, n_init=1)\n",
    "\n",
    "#X_mds = mds.fit(sim_matrix).embedding_\n",
    "\n",
    "sim_matrix = jaccard_df.pivot(\n",
    "    index=\"cluster_pair_1\", columns=\"cluster_pair_2\", values=\"weighted_jaccard\"\n",
    ")\n",
    "\n",
    "sim_matrix = sim_matrix.sort_index(axis=0).sort_index(axis=1)\n",
    "#np.fill_diagonal(sim_matrix.values, 1)\n",
    "dist_matrix = 1 - sim_matrix\n",
    "\n",
    "\n",
    "new_sim = sim_matrix.copy()\n",
    "for i in sim_matrix.index:\n",
    "    #print(i)\n",
    "    if i in MF[\"cluster_pair\"].values:\n",
    "        new_sim.loc[i, \"dataset\"] = \"MF\"\n",
    "    elif i in RCC[\"cluster_pair\"].values: \n",
    "        new_sim.loc[i, \"dataset\"] = \"RCC\"\n",
    "    else:\n",
    "        new_sim.loc[i, \"dataset\"] = \"mixed\"\n",
    "    \n",
    "jacc_dist_transform = mds.fit_transform(dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = MDS(dissimilarity=\"precomputed\", random_state=0, max_iter=200, metric=False, n_init=1)\n",
    "jacc_dist_transform = mds.fit_transform(dist_matrix.sort_index().sort_index(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame({\n",
    "    \"x\"   : jacc_dist_transform[:, 0],\n",
    "    \"y\"   : jacc_dist_transform[:, 1],\n",
    "    \"label\": dist_matrix.index\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(40, 40))    \n",
    "sns.scatterplot(data=plot_df, x=\"x\", y=\"y\",\n",
    "                s=120, ax=ax)     \n",
    "\n",
    "\n",
    "for _, row in plot_df.iterrows():\n",
    "    ax.annotate(\n",
    "        row[\"label\"],                       \n",
    "        xy=(row[\"x\"], row[\"y\"]),           \n",
    "        xytext=(3, 3), textcoords=\"offset points\", \n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Dim 1\")\n",
    "ax.set_ylabel(\"Dim 2\")\n",
    "ax.legend(title=\"Sample\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame({\n",
    "    \"x\"   : jacc_dist_transform[:, 0],\n",
    "    \"y\"   : jacc_dist_transform[:, 1],\n",
    "    \"label\": dist_matrix.index,\n",
    "    \"dataset\": new_sim[\"dataset\"]\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(40, 40))    \n",
    "sns.scatterplot(data=plot_df, x=\"x\", y=\"y\",\n",
    "                hue=\"dataset\", s=120, ax=ax)     \n",
    "\n",
    "\n",
    "for _, row in plot_df.iterrows():\n",
    "    ax.annotate(\n",
    "        row[\"label\"],                       \n",
    "        xy=(row[\"x\"], row[\"y\"]),           \n",
    "        xytext=(3, 3), textcoords=\"offset points\", \n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Dim 1\")\n",
    "ax.set_ylabel(\"Dim 2\")\n",
    "ax.legend(title=\"Sample\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prince\n",
    "\n",
    "dist_matrix_rect = 1 - heatrmap_df\n",
    "ca = prince.CA(n_components=2)\n",
    "ca = ca.fit(dist_matrix)\n",
    "#  Extract row and column coordinates\n",
    "row_coords = ca.row_coordinates(dist_matrix)\n",
    "col_coords = ca.column_coordinates(dist_matrix)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(100, 100))\n",
    "plt.scatter(row_coords[0], row_coords[1], c='red', label='Rows')\n",
    "plt.scatter(col_coords[0], col_coords[1], c='blue', label='Columns')\n",
    "\n",
    "# Adding labels\n",
    "for i, txt in enumerate(dist_matrix_rect.index):\n",
    "    plt.annotate(txt, (row_coords[0][i], row_coords[1][i]), color='red')\n",
    "\n",
    "for i, txt in enumerate(dist_matrix_rect.columns):\n",
    "    plt.annotate(txt, (col_coords[0][i], col_coords[1][i]), color='blue')\n",
    "\n",
    "plt.title('Correspondence Analysis')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix_comp = jaccard_df_comp.pivot(columns= \"RCC_cluster\", index=\"MF_cluster\", values= \"weighted_jaccard\")\n",
    "dist_matrix_comp = 1 - sim_matrix_comp\n",
    "\n",
    "\n",
    "import umap\n",
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(dist_matrix_comp)\n",
    "embedding.shape\n",
    "\n",
    "plt.figure(figsize=(30, 30), dpi=400)\n",
    "\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    s = 0.1,\n",
    "    )\n",
    "\n",
    "embedding_df = pd.DataFrame(embedding, columns=[\"UMAP1\", \"UMAP2\"])\n",
    "embedding_df[\"cellpair\"] = np.array(dist_matrix_comp.index + dist_matrix_comp.columns)\n",
    "\n",
    "for i, row in embedding_df.iterrows():\n",
    "    plt.text(row['UMAP1'], row['UMAP2'], row['cellpair'], fontsize=0.05)\n",
    "\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP Jaccard Distances', fontsize=15);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
