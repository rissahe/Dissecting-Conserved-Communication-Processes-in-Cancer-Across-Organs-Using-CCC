{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.patches as patches\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import pairwise_distances, silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "import leidenalg\n",
    "import igraph as ig\n",
    "import itertools\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import colors as mcolors\n",
    "from matplotlib.patches import Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT full feature (only comparing between datasets)\n",
    "#bipartite matrix\n",
    "\n",
    "#RCC = pd.read_csv(\"/home/larissa/Documents/Masterarbeit/RCC_results/liana_log_norm/no_subset_glom_removed/T_lr_ready.csv\", index_col=0)\n",
    "#MF = pd.read_csv(\"/home/larissa/Documents/Masterarbeit/MF_results/liana/all_celltypes/MF_lr_ready.csv\", index_col=0)\n",
    "\n",
    "#RCC = pd.read_csv(\"D:\\\\studium\\\\Masterarbeit\\\\RCC_results\\\\liana_log_norm\\\\no_subset_glom_removed\\\\T_lr_ready.csv\", index_col=0)\n",
    "#MF = pd.read_csv(\"D:\\\\studium\\\\Masterarbeit\\\\MF_results\\\\liana\\\\all_celltypes\\\\MF_lr_ready.csv\", index_col=0)\n",
    "\n",
    "RCC = pd.read_csv(\"C:\\\\Users\\\\laris\\\\Documents\\\\Studium\\\\Masterarbeit\\\\RCC_results\\\\liana_log_norm\\\\no_subset_glom_removed\\\\T_lr_ready.csv\", index_col = 0)\n",
    "MF = pd.read_csv(\"C:\\\\Users\\\\laris\\\\Documents\\\\Studium\\\\Masterarbeit\\\\MF_results\\\\liana\\\\all_celltypes\\\\MF_lr_ready.csv\", index_col = 0)\n",
    "\n",
    "RCC[\"lr_pair\"] = RCC[\"gene_A\"] + \"_\" + RCC[\"gene_B\"]\n",
    "MF[\"lr_pair\"] = MF[\"gene_A\"] + \"_\" + MF[\"gene_B\"]\n",
    "\n",
    "RCC[\"cluster_pair\"] = RCC[\"source\"] + \"@\" + RCC[\"target\"]\n",
    "MF[\"cluster_pair\"] = MF[\"source\"] + \"@\" + MF[\"target\"]\n",
    "\n",
    "RCC[\"lr_means_normed\"] = (RCC[\"lr_means\"]-RCC[\"lr_means\"].min())/(RCC[\"lr_means\"].max()-RCC[\"lr_means\"].min())\n",
    "MF[\"lr_means_normed\"] = (MF[\"lr_means\"]-MF[\"lr_means\"].min())/(MF[\"lr_means\"].max()-MF[\"lr_means\"].min())\n",
    "\n",
    "#lr_threshold\n",
    "RCC_filtered = RCC[(RCC[\"lr_means\"] >= 0.0) | (RCC[\"lr_means\"] <=  -abs(0.0))]\n",
    "MF_filtered = MF[(MF[\"lr_means\"] >= 0.0) | (MF[\"lr_means\"] <= -abs(0.0))]\n",
    "\n",
    "RCC_grouped = (RCC_filtered.groupby(\"cluster_pair\")\n",
    "               .apply(lambda df: dict(zip(df[\"lr_pair\"], df[\"lr_means_normed\"])))\n",
    "               .to_dict())\n",
    "\n",
    "MF_grouped = (MF_filtered.groupby(\"cluster_pair\")\n",
    "              .apply(lambda df: dict(zip(df[\"lr_pair\"], df[\"lr_means_normed\"])))\n",
    "              .to_dict())\n",
    "\n",
    "# remove cell pairs that only have 5 or less unique LR pairs \n",
    "RCC_grouped = {k: v for k, v in RCC_grouped.items() if len(v) >= 5}\n",
    "MF_grouped = {k: v for k, v in MF_grouped.items() if len(v) >= 5}\n",
    "\n",
    "def weighted_jaccard_dict(dict1, dict2):\n",
    "    all_keys = set(dict1.keys()).union(dict2.keys())\n",
    "    vec1 = np.array([dict1.get(i, 0.0) for i in all_keys])\n",
    "    vec2 = np.array([dict2.get(i, 0.0) for i in all_keys])\n",
    "    return np.minimum(vec1, vec2).sum() / np.maximum(vec1, vec2).sum() if np.maximum(vec1, vec2).sum() != 0 else 0.0\n",
    "\n",
    "# pairwise similarity\n",
    "results = []\n",
    "for MF_cluster, MF_dict in MF_grouped.items():\n",
    "    for RCC_cluster, RCC_dict in RCC_grouped.items():\n",
    "        score = weighted_jaccard_dict(MF_dict, RCC_dict)\n",
    "        results.append((MF_cluster, RCC_cluster, score))\n",
    "\n",
    "jaccard_df_comp = pd.DataFrame(results, columns=[\"MF_cluster\", \"RCC_cluster\", \"weighted_jaccard\"])\n",
    "#jaccard_df.to_csv(\"jaccard_score_weighted_cluster_pairs_thresholded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of cellpairs with this number of unique LR pairs\n",
    "\n",
    "RCC_filtered.groupby(\"cluster_pair\")[\"lr_pair\"].size().hist(bins=200, grid = False)\n",
    "#plt.xticks(ticks=[0, 10, 20,  30, 40, 50, 60, 70, 80, 85, 90, 95, 100],)\n",
    "plt.xlabel(\"Amount of unique LR pairs in one cellpair []\")\n",
    "plt.ylabel(\"Number of cellpairs with this number of unique LR pairs []\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of cellpairs with this number of unique LR pairs small\n",
    "\n",
    "RCC_filtered.groupby(\"cluster_pair\")[\"lr_pair\"].size().hist(bins=200, grid = False)\n",
    "plt.xlim(0, 25)\n",
    "plt.xlabel(\"Amount of unique LR pairs in one cellpair []\")\n",
    "plt.ylabel(\"Number of cellpairs with this number of unique LR pairs []\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((RCC_filtered.groupby(\"cluster_pair\")[\"lr_pair\"].size() < 5).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of cellpairs with this number of unique LR pairs\n",
    "\n",
    "MF_filtered.groupby(\"cluster_pair\")[\"lr_pair\"].size().hist(bins=200, grid = False)\n",
    "#plt.xticks(ticks=[0, 10, 20,  30, 40, 50, 60, 70, 80, 85, 90, 95, 100],)\n",
    "plt.xlabel(\"Amount of unique LR pairs in one cellpair []\")\n",
    "plt.ylabel(\"Number of cellpairs with this number of unique LR pairs []\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of cellpairs with this number of unique LR pairs small\n",
    "\n",
    "MF_filtered.groupby(\"cluster_pair\")[\"lr_pair\"].size().hist(bins=200, grid = False)\n",
    "plt.xlim(0, 25)\n",
    "plt.xlabel(\"Amount of unique LR pairs in one cellpair\")\n",
    "plt.ylabel(\"Number of cellpairs with this number of unique LR pairs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(MF_filtered.groupby(\"cluster_pair\")[\"lr_pair\"].size() < 5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jaccard similarity distribution\n",
    "\n",
    "jaccard_df_comp_dist = 1 - jaccard_df_comp[\"weighted_jaccard\"]\n",
    "ax = sns.violinplot(jaccard_df_comp, inner= \"quart\")\n",
    "quant = np.quantile(jaccard_df_comp[\"weighted_jaccard\"], 0.75)\n",
    "print(quant)\n",
    "jaccard_df_quant = jaccard_df_comp[jaccard_df_comp[\"weighted_jaccard\"] > quant]\n",
    "ax.set_xticklabels([])\n",
    "plt.xlabel(\"Weighted Jaccard Similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bipartite matrix heatmap\n",
    "\n",
    "heatrmap_df = jaccard_df_comp.pivot(\n",
    "    index=\"RCC_cluster\", columns=\"MF_cluster\", values=\"weighted_jaccard\"\n",
    ")\n",
    "\n",
    "heatrmap_df = 1- heatrmap_df\n",
    "clust = sns.clustermap(heatrmap_df, figsize=(30, 30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster hierarchically\n",
    "\n",
    "heatrmap_df = jaccard_df_comp.pivot(index=\"RCC_cluster\", columns=\"MF_cluster\", values=\"weighted_jaccard\")\n",
    "heatrmap_df = 1 - heatrmap_df\n",
    "\n",
    "g = sns.clustermap(heatrmap_df, method='average', figsize = (30,30), yticklabels=0, xticklabels=0, \n",
    "                  cbar_pos=(0.12, 0.82, 0.03, 0.15))\n",
    "\n",
    "g.ax_heatmap.set_xlabel(\"RCC cluster\", fontsize=30)\n",
    "g.ax_heatmap.set_ylabel(\"MF cluster\", fontsize=30)\n",
    "cbar = g.ax_heatmap.collections[0].colorbar\n",
    "cbar.set_label(\"Jaccard distance\", fontsize=30)\n",
    "cbar.ax.tick_params(labelsize=25)    \n",
    "cbar.ax.yaxis.set_label_position(\"left\")\n",
    "\n",
    "row_order = g.dendrogram_row.reordered_ind\n",
    "col_order = g.dendrogram_col.reordered_ind\n",
    "reordered_rows = [heatrmap_df.index[i] for i in row_order]\n",
    "reordered_cols = [heatrmap_df.columns[i] for i in col_order]\n",
    "\n",
    "\n",
    "#define dendrogram clustering threshold\n",
    "row_clusters = fcluster(g.dendrogram_row.linkage, t=11, criterion='maxclust')\n",
    "col_clusters = fcluster(g.dendrogram_col.linkage, t=12, criterion='maxclust')\n",
    "\n",
    "# map to reordered labels\n",
    "row_cluster_labels = pd.Series(row_clusters, index=heatrmap_df.index).loc[reordered_rows]\n",
    "col_cluster_labels = pd.Series(col_clusters, index=heatrmap_df.columns).loc[reordered_cols]\n",
    "\n",
    "ax = g.ax_heatmap\n",
    "\n",
    "# find start and length of each cluster in order\n",
    "def get_cluster_spans(cluster_labels):\n",
    "    spans = []\n",
    "    prev_label = None\n",
    "    start_idx = 0\n",
    "    for idx, label in enumerate(cluster_labels):\n",
    "        if label != prev_label and prev_label is not None:\n",
    "            spans.append((prev_label, start_idx, idx))\n",
    "            start_idx = idx\n",
    "        prev_label = label\n",
    "    spans.append((prev_label, start_idx, len(cluster_labels)))\n",
    "    return spans\n",
    "\n",
    "def enforce_min_spacing(positions, min_dist):\n",
    "    adjusted = [positions[0]]\n",
    "    for p in positions[1:]:\n",
    "        if p - adjusted[-1] < min_dist:\n",
    "            p = adjusted[-1] + min_dist\n",
    "        adjusted.append(p)\n",
    "    return adjusted\n",
    "\n",
    "\n",
    "row_spans = get_cluster_spans(row_cluster_labels.tolist())\n",
    "row_centers = [(start + (end - start) / 2 - 0.5) for (_, start, end) in row_spans]\n",
    "row_centers_adj = enforce_min_spacing(row_centers, min_dist=13)\n",
    "\n",
    "\n",
    "# row cluster spans (y-axis)\n",
    "\n",
    "for (label, start, end), y_adj in zip(row_spans, row_centers_adj):\n",
    "    height = end - start\n",
    "    rect = patches.Rectangle(\n",
    "        xy=(-0.5, start - 0.5),\n",
    "        width=len(col_cluster_labels),\n",
    "        height=height,\n",
    "        fill=False,\n",
    "        edgecolor='blue',\n",
    "        linewidth=2\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(\n",
    "        x=-1.5,\n",
    "        #y=start + height / 2 - 0.5,\n",
    "        y=y_adj,\n",
    "        s=f\"RCC{label}\",\n",
    "        va='center',\n",
    "        ha='right',\n",
    "        fontsize=25,\n",
    "        color='blue'\n",
    "    )\n",
    "\n",
    "\n",
    "# col cluster spans(x axis)\n",
    "col_spans = get_cluster_spans(col_cluster_labels.tolist())\n",
    "num_cols = len(col_cluster_labels)\n",
    "\n",
    "# normalized centers \n",
    "col_centers = [(start + (end - start)/2) / num_cols for (_, start, end) in col_spans]\n",
    "\n",
    "# enforce minimum spacing\n",
    "def enforce_min_spacing_axes(positions, min_dist=1):\n",
    "    adjusted = [positions[0]]\n",
    "    for p in positions[1:]:\n",
    "        if p - adjusted[-1] < min_dist:\n",
    "            p = adjusted[-1] + min_dist\n",
    "        adjusted.append(p)\n",
    "    return adjusted\n",
    "\n",
    "col_centers_adj = enforce_min_spacing_axes(col_centers, min_dist=0.03)\n",
    "\n",
    "for (label, start, end), x_adj in zip(col_spans, col_centers_adj):\n",
    "    # rectangle in data coordinates\n",
    "    rect = patches.Rectangle(\n",
    "        xy=(start - 0.5, -0.5),\n",
    "        width=end - start,\n",
    "        height=len(row_cluster_labels),\n",
    "        fill=False,\n",
    "        edgecolor='green',\n",
    "        linewidth=2\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    # label in axes coordinates\n",
    "    ax.text(\n",
    "        x=x_adj,\n",
    "        y= 1.02,               \n",
    "        transform=ax.transAxes,\n",
    "        s=f\"MF{label}\",\n",
    "        ha='center',\n",
    "        va='top',\n",
    "        fontsize=25,\n",
    "        color='green',\n",
    "        rotation=0\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_cluster_labels_df = pd.DataFrame(row_cluster_labels)\n",
    "row_cluster_labels_df['RCC_cluster'] = row_cluster_labels_df.index\n",
    "row_cluster_labels_df = pd.DataFrame(row_cluster_labels).rename(columns={0: \"row_cluster\"})\n",
    "\n",
    "col_cluster_labels_df = pd.DataFrame(col_cluster_labels)\n",
    "col_cluster_labels_df['MF_cluster'] = col_cluster_labels_df.index\n",
    "col_cluster_labels_df = pd.DataFrame(col_cluster_labels).rename(columns={0: \"col_cluster\"})\n",
    "\n",
    "jaccard_df_compp = jaccard_df_comp.merge(row_cluster_labels_df, on=\"RCC_cluster\")\n",
    "jaccard_df_compp = jaccard_df_compp.merge(col_cluster_labels_df, on=\"MF_cluster\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heatmap = (\n",
    "    jaccard_df_compp\n",
    "    .groupby([\"row_cluster\", \"col_cluster\"])[\"weighted_jaccard\"]\n",
    "    .median()\n",
    "    .reset_index()\n",
    "    .pivot(index=\"row_cluster\", columns=\"col_cluster\", values=\"weighted_jaccard\")\n",
    ")\n",
    "df_heatmap = 1- df_heatmap\n",
    "df_heatmap = df_heatmap.where(df_heatmap <0.90, 0)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "ax = sns.heatmap(df_heatmap, annot=True, cmap=\"viridis\")\n",
    "\n",
    "ax.set_xlabel(\"MF Clusters\")\n",
    "ax.set_ylabel(\"RCC CLusters\")\n",
    "\n",
    "# custom tick labels\n",
    "#ax.set_xticklabels([\"label1\", \"label2\", \"label3\"], rotation=45, ha=\"right\")\n",
    "#ax.set_yticklabels([\"row1\", \"row2\", \"row3\"], rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores between all cell cell pairs + cluster assignments\n",
    "\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "for i in jaccard_df_compp[\"row_cluster\"].sort_values().unique():\n",
    "    for j in jaccard_df_compp[\"col_cluster\"].sort_values().unique():\n",
    "        new_df_tmp = jaccard_df_compp[(jaccard_df_compp[\"row_cluster\"] == i) & (jaccard_df_compp[\"col_cluster\"] == j)] \n",
    "        new_df_tmp = new_df_tmp.sort_values(by=\"weighted_jaccard\", ascending=False).head(n=15)\n",
    "        new_df = pd.concat([new_df, new_df_tmp])\n",
    "\n",
    "\n",
    "for i, row in new_df.iterrows():\n",
    "    value = df_heatmap.loc[row[\"row_cluster\"], row[\"col_cluster\"]]\n",
    "    new_df.at[i, \"median_jaccard\"] = value\n",
    "\n",
    "filtered  = new_df[(new_df[\"median_jaccard\"] != 0)]\n",
    "\n",
    "new_df.to_csv(\"all_jaccard_cellpairs_min_5_lr_interactions_all.csv\", index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_cluster_labels_df = pd.DataFrame(row_cluster_labels)\n",
    "row_cluster_labels_df = pd.DataFrame(row_cluster_labels).rename(columns={0: \"row_cluster\"})\n",
    "row_cluster_labels_df['RCC_cluster'] = row_cluster_labels_df.index\n",
    "\n",
    "\n",
    "col_cluster_labels_df = pd.DataFrame(col_cluster_labels)\n",
    "col_cluster_labels_df = pd.DataFrame(col_cluster_labels).rename(columns={0: \"col_cluster\"})\n",
    "col_cluster_labels_df['MF_cluster'] = col_cluster_labels_df.index\n",
    "\n",
    "\n",
    "row_cluster_labels_df[[\"RCC_Sender\", \"RCC_Receiver\"]] = row_cluster_labels_df[\"RCC_cluster\"].str.split(\"@\", expand= True)\n",
    "r_sender_counts = row_cluster_labels_df.groupby([\"row_cluster\", \"RCC_Sender\"]).size().unstack(fill_value=0)\n",
    "r_receiver_counts = row_cluster_labels_df.groupby([\"row_cluster\", \"RCC_Receiver\"]).size().unstack(fill_value=0)\n",
    "r_sender_counts2 = r_sender_counts.add_suffix(\"_as_sender\")\n",
    "r_receiver_counts2 = r_receiver_counts.add_suffix(\"_as_receiver\")\n",
    "row_combined_counts = pd.concat([r_sender_counts2, r_receiver_counts2], axis=1).fillna(0).astype(int)\n",
    "row_combined_counts = row_combined_counts.T\n",
    "\n",
    "col_cluster_labels_df[[\"MF_Sender\", \"MF_Receiver\"]] = col_cluster_labels_df[\"MF_cluster\"].str.split(\"@\", expand= True)\n",
    "c_sender_counts = col_cluster_labels_df.groupby([\"col_cluster\", \"MF_Sender\"]).size().unstack(fill_value=0)\n",
    "c_receiver_counts = col_cluster_labels_df.groupby([\"col_cluster\", \"MF_Receiver\"]).size().unstack(fill_value=0)\n",
    "c_sender_counts2 = c_sender_counts.add_suffix(\"_as_sender\")\n",
    "c_receiver_counts2 = c_receiver_counts.add_suffix(\"_as_receiver\")\n",
    "col_combined_counts = pd.concat([c_sender_counts2, c_receiver_counts2], axis=1).fillna(0).astype(int)\n",
    "col_combined_counts = col_combined_counts.T\n",
    "\n",
    "c_sender_counts = c_sender_counts.T\n",
    "c_receiver_counts = c_receiver_counts.T\n",
    "\n",
    "r_sender_counts = r_sender_counts.T\n",
    "r_receiver_counts = r_receiver_counts.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts of cell types per cluster\n",
    "\n",
    "lst = []\n",
    "\n",
    "for i in row_combined_counts.columns:\n",
    "    for j in col_combined_counts.columns:\n",
    "\n",
    "        value = df_heatmap.loc[i, j]\n",
    "        if value == 0:\n",
    "                continue\n",
    "        \n",
    "        r_s = pd.DataFrame()\n",
    "        r_r = pd.DataFrame()\n",
    "        c_s = pd.DataFrame()\n",
    "        c_r = pd.DataFrame()\n",
    "\n",
    "        r_s[\"counts\"] = r_sender_counts[i]\n",
    "        r_s[\"percentage\"] = (r_s[\"counts\"]/(r_s[\"counts\"].sum()))*100\n",
    "\n",
    "        r_r[\"counts\"] = r_receiver_counts[i]\n",
    "        r_r[\"percentage\"] = (r_r[\"counts\"]/(r_r[\"counts\"].sum()))*100\n",
    "\n",
    "        c_s[\"counts\"] = c_sender_counts[j]\n",
    "        c_s[\"percentage\"] = (c_s[\"counts\"]/(c_s[\"counts\"].sum()))*100\n",
    "\n",
    "        c_r[\"counts\"] = c_receiver_counts[j]\n",
    "        c_r[\"percentage\"] = (c_r[\"counts\"]/(c_r[\"counts\"].sum()))*100\n",
    "\n",
    "\n",
    "        sorted_r_s = r_s.sort_values(by=\"counts\", ascending=False).head(n=5)\n",
    "        sorted_c_s = c_s.sort_values(by=\"counts\", ascending=False).head(n=5)\n",
    "        sorted_r_r = r_r.sort_values(by=\"counts\", ascending=False).head(n=5)\n",
    "        sorted_c_r = c_r.sort_values(by=\"counts\", ascending=False).head(n=5)\n",
    "\n",
    "        sorted_r_s = sorted_r_s[sorted_r_s[\"counts\"]!=0]\n",
    "        sorted_c_s = sorted_c_s[sorted_c_s[\"counts\"]!=0]\n",
    "        sorted_r_r = sorted_r_r[sorted_r_r[\"counts\"]!=0]\n",
    "        sorted_c_r = sorted_c_r[sorted_c_r[\"counts\"]!=0]\n",
    "\n",
    "        lst.append(f\"Cluster Similarity {i} and {j}: {df_heatmap.loc[i, j]} \\n Total number of cellpairs: {r_s[\"counts\"].sum()} (RCC); {c_s[\"counts\"].sum()} (MF) \\n Unique Senders: {len(r_s.index[r_s[\"counts\"] != 0].unique())} (RCC), {len(c_s.index[c_s[\"counts\"] != 0].unique())} (MF) \\n Unique Receivers: {len(r_r.index[r_r[\"counts\"] != 0].unique())} (RCC), {len(c_r.index[c_r[\"counts\"] != 0].unique())} (MF)\")\n",
    "        lst.append(sorted_r_s)\n",
    "        lst.append(sorted_c_s)\n",
    "        lst.append(sorted_r_r)\n",
    "        lst.append(sorted_c_r)\n",
    "        \n",
    "\n",
    "lst\n",
    "with open(\"top5_celltypes_per_cluster_dataset_comparison_normalized_LR_scores.txt\", 'w') as f:\n",
    "    for s in lst:\n",
    "        f.write(str(s) + '\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RCC = RCC[RCC[\"lr_means_normed\"] > 0.10]\n",
    "#MF = MF[MF[\"lr_means_normed\"] > 0.10]\n",
    "\n",
    "mean_LR_RCC = RCC.groupby([\"source\", \"target\"])[\"lr_means_normed\"].mean()\n",
    "mean_LR_RCC.add_prefix(\"RCC_\", axis=0)\n",
    "mean_LR_MF = MF.groupby([\"source\", \"target\"])[\"lr_means_normed\"].mean()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.violinplot(mean_LR_RCC, inner=\"quart\", color=  \"#FF944C\")\n",
    "print(np.quantile(mean_LR_RCC, 0.75))\n",
    "print(np.quantile(mean_LR_MF, 0.75))\n",
    "sns.violinplot(mean_LR_MF, inner=\"quart\", color = \"#94FF8F\")\n",
    "ax.set_ylabel(\"LR Means Normed\")\n",
    "\n",
    "\n",
    "mean_LR_RCC = pd.DataFrame(mean_LR_RCC).reset_index()\n",
    "mean_LR_RCC = mean_LR_RCC.pivot(index = \"source\", columns = \"target\", values = \"lr_means_normed\")\n",
    "mean_LR_RCC = mean_LR_RCC.sort_index(axis=0).sort_index(axis=1).add_prefix(\"RCC_\", axis=0).add_prefix(\"RCC_\", axis=1)\n",
    "\n",
    "mean_LR_MF = pd.DataFrame(mean_LR_MF).reset_index()\n",
    "mean_LR_MF = mean_LR_MF.pivot(index = \"source\", columns = \"target\", values = \"lr_means_normed\")\n",
    "mean_LR_MF = mean_LR_MF.sort_index(axis=0).sort_index(axis=1).add_prefix(\"MF_\", axis=0).add_prefix(\"MF_\", axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sender to sender and receiver to receiver\n",
    "jaccard_df_comp[[\"MF_sender\", \"MF_receiver\"]] = jaccard_df_comp[\"MF_cluster\"].str.split(\"@\", expand=True)\n",
    "jaccard_df_comp[[\"RCC_sender\", \"RCC_receiver\"]] = jaccard_df_comp[\"RCC_cluster\"].str.split(\"@\", expand=True)\n",
    "\n",
    "jaccard_df_comp_split_s = jaccard_df_comp.groupby([\"MF_sender\", \"RCC_sender\"])[\"weighted_jaccard\"].mean()\n",
    "jaccard_df_comp_split_r = jaccard_df_comp.groupby([\"RCC_receiver\", \"MF_receiver\"])[\"weighted_jaccard\"].mean()\n",
    "\n",
    "jaccard_df_comp_split_s = jaccard_df_comp_split_s.reset_index()\n",
    "jaccard_df_comp_split_s = jaccard_df_comp_split_s.pivot(index = \"RCC_sender\", columns = \"MF_sender\", values= \"weighted_jaccard\").add_prefix(\"RCC_\", axis=0).add_prefix(\"MF_\", axis=1)\n",
    "\n",
    "jaccard_df_comp_split_r = jaccard_df_comp_split_r.reset_index()\n",
    "jaccard_df_comp_split_r = jaccard_df_comp_split_r.pivot(index = \"MF_receiver\", columns = \"RCC_receiver\", values= \"weighted_jaccard\").add_prefix(\"MF_\", axis=0).add_prefix(\"RCC_\", axis=1)\n",
    "jaccard_df_comp_split = pd.concat((jaccard_df_comp_split_s, jaccard_df_comp_split_r))\n",
    "\n",
    "print(np.quantile(jaccard_df_comp_split_s, 0.75))\n",
    "print(np.quantile(jaccard_df_comp_split_r, 0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sender and receiver probabilities\n",
    "\n",
    "jaccard_df_comp_split_s_p = jaccard_df_comp_split_s.div(jaccard_df_comp_split_s.max(axis=1), axis=0) \n",
    "jaccard_df_comp_split_r_p = jaccard_df_comp_split_r.div(jaccard_df_comp_split_r.max(axis=1), axis=0) \n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.violinplot(jaccard_df_comp_split_s_p.mean(), inner=\"quart\", color=  \"#FF944C\")\n",
    "sns.violinplot(jaccard_df_comp_split_r_p.mean(), inner=\"quart\", color = \"#94FF8F\")\n",
    "\n",
    "print(np.quantile(jaccard_df_comp_split_s_p, 0.75))\n",
    "print(np.quantile(jaccard_df_comp_split_r_p, 0.75))\n",
    "\n",
    "jaccard_df_comp_split_s_p = jaccard_df_comp_split_s_p[jaccard_df_comp_split_s_p >= 0.7]\n",
    "jaccard_df_comp_split_r_p = jaccard_df_comp_split_r_p[jaccard_df_comp_split_r_p >= 0.62]\n",
    "\n",
    "jaccard_df_comp_split_p = pd.concat((jaccard_df_comp_split_s_p, jaccard_df_comp_split_r_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_df_comp_split_m = jaccard_df_comp_split_p\n",
    "combined_matrices = pd.DataFrame()\n",
    "\n",
    "median_LR_MF = median_LR_MF[median_LR_MF >=0.23958727957045906]\n",
    "median_LR_RCC = median_LR_RCC[median_LR_RCC >=0.2265910763666835]\n",
    "\n",
    "combined_matrices_LR = pd.concat((median_LR_MF, median_LR_RCC))\n",
    "\n",
    "#thresholds\n",
    "#combined_matrices_LR = combined_matrices_LR[combined_matrices_LR >= mean_quants]#0.23]\n",
    "#jaccard_df_comp_split_m = jaccard_df_comp_split_m[jaccard_df_comp_split_m >= 0.65]\n",
    "\n",
    "#setting Jaccard score to 1\n",
    "#jaccard_df_comp_split_m = jaccard_df_comp_split_m.mask(jaccard_df_comp_split_m < 0.05)\n",
    "#jaccard_df_comp_split_m = jaccard_df_comp_split_m.applymap(lambda x: 1 if pd.notna(x) else np.nan)\n",
    "\n",
    "#normalizing Jaccard and LR before merging to be on same scale\n",
    "#min_val = combined_matrices_LR.min()\n",
    "#max_val = combined_matrices_LR.max()\n",
    "#combined_matrices_LR = (combined_matrices_LR - min_val) / (max_val - min_val)\n",
    "#min_val = jaccard_df_comp_split_m.min()\n",
    "#max_val = jaccard_df_comp_split_m.max()\n",
    "#jaccard_df_comp_split_m = (jaccard_df_comp_split_m - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "\n",
    "combined_matrices = combined_matrices_LR.combine_first(jaccard_df_comp_split_m)\n",
    "combined_matrices = combined_matrices.fillna(0)\n",
    "\n",
    "\n",
    "g_comb = ig.Graph.Weighted_Adjacency(\n",
    "    combined_matrices.to_numpy().tolist(), \n",
    "    mode= \"directed\",     \n",
    "    attr=\"weight\"\n",
    ")\n",
    "\n",
    "#node names\n",
    "g_comb.vs[\"name\"] = list(combined_matrices.index)\n",
    "g_comb.vs[\"label\"] = g_comb.vs[\"name\"]\n",
    "g_comb.vs[\"label_size\"] = 10\n",
    "degree = g_comb.degree()\n",
    "\n",
    "clusters_comb = leidenalg.find_partition(g_comb, weights=g_comb.es[\"weight\"], partition_type= leidenalg.CPMVertexPartition, resolution_parameter= 0.27,seed = 33)\n",
    "print(clusters_comb)\n",
    "#g_comb.vs[\"color\"] = [clusters_comb.membership[i] for i in range(len(g_comb.vs))]\n",
    "\n",
    "layout = g_comb.layout(\"fr\") \n",
    "\n",
    "\n",
    "\n",
    "ig.plot(clusters_comb, layout = layout, vertex_size=30, edge_width=[w[\"weight\"] for w in g_comb.es], edge_alpha= 0.4, target=\"clustered_igraph_quant.png\", \n",
    "        bbox = (1500,1500), margin = 100)\n",
    "\n",
    "#n = g_comb.to_networkx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CPM Vertex  k per resolution plot\n",
    "\n",
    "resolutions = np.linspace(0, 0.4, 100) \n",
    "results_res = []\n",
    "results_k = []\n",
    "results = []\n",
    "\n",
    "runs = 100\n",
    "\n",
    "for r in resolutions:\n",
    "    aris = []\n",
    "    nmi = []\n",
    "    partition = leidenalg.find_partition(\n",
    "        g_comb,\n",
    "        leidenalg.CPMVertexPartition,\n",
    "        weights=g_comb.es[\"weight\"],\n",
    "        resolution_parameter=r,\n",
    "        seed = 33\n",
    "    )\n",
    "    modularity = partition.modularity\n",
    "    results_res.append(r)\n",
    "    results_k.append(len(partition))\n",
    "    results.append((r, len(partition)))\n",
    "\n",
    "#plt.bar(results_res, results_k, align=\"edge\")\n",
    "#plt.xticks(results_res)\n",
    "#plt.yticks(results_k)\n",
    "#plt.xlim(0,0.4)\n",
    "\n",
    "resultsdf = pd.DataFrame(results)\n",
    "ax = sns.lineplot(x = resultsdf[0], y = resultsdf[1])\n",
    "ax.set(xlabel = \"CPMVertex Resolutions\", ylabel = \"k clusters\", yticks = results_k)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RBERVertex  k per resolution plot\n",
    "resolutions = np.linspace(0.5, 3, 300) \n",
    "results_res = []\n",
    "results_k = []\n",
    "results = []\n",
    "\n",
    "runs = 100\n",
    "\n",
    "for r in resolutions:\n",
    "    aris = []\n",
    "    nmi = []\n",
    "    partition = leidenalg.find_partition(\n",
    "        g_comb,\n",
    "        leidenalg.RBERVertexPartition,\n",
    "        weights=g_comb.es[\"weight\"],\n",
    "        resolution_parameter=r,\n",
    "        seed = 33\n",
    "    )\n",
    "    modularity = partition.modularity\n",
    "    results_res.append(r)\n",
    "    results_k.append(len(partition))\n",
    "    results.append((r, len(partition)))\n",
    "\n",
    "\n",
    "#plt.bar(results_res, results_k, align=\"edge\")\n",
    "#plt.xticks(results_res)\n",
    "#plt.yticks(results_k)\n",
    "#plt.xlim(0.5, 3)\n",
    "\n",
    "resultsdf = pd.DataFrame(results)\n",
    "ax = sns.lineplot(x = resultsdf[0], y = resultsdf[1])\n",
    "ax.set(xlabel = \"RBERVertex Resolutions\", ylabel = \"k clusters\", yticks = results_k)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RBERVertex\n",
    "\n",
    "jaccard_df_comp_split_m = jaccard_df_comp_split_p\n",
    "combined_matrices = pd.DataFrame()\n",
    "\n",
    "\n",
    "median_LR_MF = median_LR_MF[median_LR_MF >= 0.2399136639826615]\n",
    "median_LR_RCC = median_LR_RCC[median_LR_RCC >= 0.2265910763666835]\n",
    "\n",
    "combined_matrices_LR = pd.concat((median_LR_MF, median_LR_RCC))\n",
    "\n",
    "#thresholds\n",
    "#combined_matrices_LR = combined_matrices_LR[combined_matrices_LR >= 0.23]\n",
    "#jaccard_df_comp_split_m = jaccard_df_comp_split_m[jaccard_df_comp_split_m >= 0.65]\n",
    "\n",
    "#setting Jaccard score to 1\n",
    "#jaccard_df_comp_split_m = jaccard_df_comp_split_m.mask(jaccard_df_comp_split_m < 0.05)\n",
    "#jaccard_df_comp_split_m = jaccard_df_comp_split_m.applymap(lambda x: 1 if pd.notna(x) else np.nan)\n",
    "\n",
    "#normalizing Jaccard and LR before merging to be on same scale\n",
    "#min_val = combined_matrices_LR.min()\n",
    "#max_val = combined_matrices_LR.max()\n",
    "#combined_matrices_LR = (combined_matrices_LR - min_val) / (max_val - min_val)\n",
    "#min_val = jaccard_df_comp_split_m.min()\n",
    "#max_val = jaccard_df_comp_split_m.max()\n",
    "#jaccard_df_comp_split_m = (jaccard_df_comp_split_m - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "\n",
    "combined_matrices = combined_matrices_LR.combine_first(jaccard_df_comp_split_m)\n",
    "combined_matrices = combined_matrices.fillna(0)\n",
    "\n",
    "\n",
    "g_comb = ig.Graph.Weighted_Adjacency(\n",
    "    combined_matrices.to_numpy().tolist(), \n",
    "    mode= \"directed\",     \n",
    "    attr=\"weight\"\n",
    ")\n",
    "\n",
    "#node names\n",
    "g_comb.vs[\"name\"] = list(combined_matrices.index)\n",
    "g_comb.vs[\"label\"] = g_comb.vs[\"name\"]\n",
    "g_comb.vs[\"label_size\"] = 10\n",
    "degree = g_comb.degree()\n",
    "\n",
    "clusters_comb = leidenalg.find_partition(g_comb, weights=g_comb.es[\"weight\"], partition_type= leidenalg.RBERVertexPartition, resolution_parameter= 1.15, seed = 33)\n",
    "print(clusters_comb)\n",
    "#g_comb.vs[\"color\"] = [clusters_comb.membership[i] for i in range(len(g_comb.vs))]\n",
    "\n",
    "layout = g_comb.layout(\"fr\") \n",
    "\n",
    "\n",
    "\n",
    "ig.plot(clusters_comb, layout = layout, vertex_size=30, edge_width=[w[\"weight\"] for w in g_comb.es], edge_alpha= 0.4, target=\"clustered_igraph_quant.png\", \n",
    "        bbox = (1500,1500), margin = 100)\n",
    "\n",
    "#n = g_comb.to_networkx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RBConfigurationVertexPartition k per resolution plot\n",
    "resolutions = np.linspace(0.5, 3, 300) \n",
    "results = []\n",
    "results_res = []\n",
    "results_k = []\n",
    "\n",
    "\n",
    "runs = 100\n",
    "\n",
    "for r in resolutions:\n",
    "    aris = []\n",
    "    nmi = []\n",
    "    partition = leidenalg.find_partition(\n",
    "        g_comb,\n",
    "        leidenalg.RBConfigurationVertexPartition,\n",
    "        weights=g_comb.es[\"weight\"],\n",
    "        resolution_parameter=r,\n",
    "        seed = 33\n",
    "    )\n",
    "    modularity = partition.modularity\n",
    "    results_res.append(r)\n",
    "    results_k.append(len(partition))\n",
    "    results.append((r, len(partition)))\n",
    "\n",
    "\n",
    "\n",
    "#plt.bar(results_res, results_k, align=\"edge\", color =\"green\")\n",
    "#plt.xticks(results_res)\n",
    "#plt.xticks(round(results_res[::35], 1))\n",
    "\n",
    "#plt.yticks(results_k)\n",
    "#plt.xlabel(\"Resolutions RBConfiguration\")\n",
    "#plt.ylabel(\"k clusters\")\n",
    "#plt.xlim(0.5, 3)\n",
    "\n",
    "resultsdf = pd.DataFrame(results)\n",
    "ax = sns.lineplot(x = resultsdf[0], y = resultsdf[1])\n",
    "ax.set(xlabel = \"RBConfiguration Resolutions\", ylabel = \"k clusters\", yticks = results_k)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RBConfigurationVertexPartition\n",
    "\n",
    "jaccard_df_comp_split_m = jaccard_df_comp_split_p\n",
    "combined_matrices = pd.DataFrame()\n",
    "\n",
    "\n",
    "mean_LR_MF = mean_LR_MF[mean_LR_MF >= 0.2399136639826615]\n",
    "mean_LR_RCC = mean_LR_RCC[mean_LR_RCC >= 0.2265910763666835]\n",
    "\n",
    "combined_matrices_LR = pd.concat((mean_LR_MF, mean_LR_RCC))\n",
    "\n",
    "#thresholds\n",
    "#combined_matrices_LR = combined_matrices_LR[combined_matrices_LR >= 0.23]\n",
    "#jaccard_df_comp_split_m = jaccard_df_comp_split_m[jaccard_df_comp_split_m >= 0.65]\n",
    "\n",
    "#setting Jaccard score to 1\n",
    "#jaccard_df_comp_split_m = jaccard_df_comp_split_m.mask(jaccard_df_comp_split_m < 0.05)\n",
    "#jaccard_df_comp_split_m = jaccard_df_comp_split_m.applymap(lambda x: 1 if pd.notna(x) else np.nan)\n",
    "\n",
    "#normalizing Jaccard and LR before merging to be on same scale\n",
    "#min_val = combined_matrices_LR.min()\n",
    "#max_val = combined_matrices_LR.max()\n",
    "#combined_matrices_LR = (combined_matrices_LR - min_val) / (max_val - min_val)\n",
    "#min_val = jaccard_df_comp_split_m.min()\n",
    "#max_val = jaccard_df_comp_split_m.max()\n",
    "#jaccard_df_comp_split_m = (jaccard_df_comp_split_m - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "\n",
    "combined_matrices = combined_matrices_LR.combine_first(jaccard_df_comp_split_m)\n",
    "combined_matrices = combined_matrices.fillna(0)\n",
    "\n",
    "\n",
    "g_comb = ig.Graph.Weighted_Adjacency(\n",
    "    combined_matrices.to_numpy().tolist(), \n",
    "    mode= \"directed\",     \n",
    "    attr=\"weight\"\n",
    ")\n",
    "\n",
    "#node names\n",
    "g_comb.vs[\"name\"] = list(combined_matrices.index)\n",
    "g_comb.vs[\"label\"] = g_comb.vs[\"name\"]\n",
    "g_comb.vs[\"label_size\"] = 10\n",
    "degree = g_comb.degree()\n",
    "\n",
    "clusters_comb = leidenalg.find_partition(g_comb, weights=g_comb.es[\"weight\"], partition_type= leidenalg.RBConfigurationVertexPartition, resolution_parameter= 1.25, seed = 33)\n",
    "print(clusters_comb)\n",
    "#g_comb.vs[\"color\"] = [clusters_comb.membership[i] for i in range(len(g_comb.vs))]\n",
    "\n",
    "layout = g_comb.layout(\"fr\") \n",
    "\n",
    "\n",
    "\n",
    "ig.plot(clusters_comb, layout = layout, vertex_size=30, edge_width=[w[\"weight\"] for w in g_comb.es], edge_alpha= 0.4, target=\"clustered_igraph_quant.png\", \n",
    "        bbox = (1500,1500), margin = 100)\n",
    "\n",
    "#n = g_comb.to_networkx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#silhoutte score clustering\n",
    "labels = clusters_comb.membership\n",
    "\n",
    "\n",
    "distances = []\n",
    "for e in g_comb.es:\n",
    "    w = e[\"weight\"]\n",
    "    dist = 1.0 / w if w > 0 else 1e9\n",
    "    distances.append(dist)\n",
    "g_comb.es[\"dist\"] = distances\n",
    "\n",
    "\n",
    "D = np.array(g_comb.distances(weights=\"dist\"))\n",
    "\n",
    "\n",
    "sil = silhouette_score(D, labels, metric=\"precomputed\")\n",
    "print(\"Silhouette score:\", sil)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RBConfig res 1.25 after removing glom endo\n",
    "cluster_dict = {\"Monocytes\" : [\"MF_CD16_monocytes\", \"MF_Classical_monocytes\", \"MF_EBM_prog\", \"MF_GMPs\",\n",
    "    \"MF_Inflammatory_CD14_monocytes\", \"MF_Late_Neutro_Prog\", \"MF_Macrophages\",\n",
    "    \"MF_Neutrophils\", \"MF_SPP1_Macrophages\", \"MF_cDC1\", \"MF_cDC2\", \"RCC_Classical monocytes\", \"RCC_Non-classical monocytes\", \"RCC_TAM 1\", \"RCC_TAM 2\", \"RCC_TAM 3\", \"RCC_TAM 4\"],\n",
    "\"Lymphocytes\" : [\"MF_B_cells\", \"MF_CD8_T_cells\",\"MF_NK_T_cells\", \"RCC_B cells\",\n",
    "    \"RCC_CD8 T cells\", \"RCC_Cytotoxic T cells\", \"RCC_IGHG-high plasma cells\",\n",
    "    \"RCC_NK cells\", \"RCC_Plasma cells\", \"RCC_Regulatory T cells\",\n",
    "    \"RCC_Resting/memory T cells\"],#, \"RCC_Proximal tubule\"],\n",
    "\"Tumor_prog\" : [\"MF_B_cell_prog\", \"MF_Plasma_cells\",\n",
    "                 \"MF_Early_Neutro_Prog\", \"MF_HSPCs\", \"MF_MEPs\", \"MF_MK_prog\",\n",
    "    \"RCC_DCT/CNT\", \"RCC_Epithelial progenitor-like cells\", \"RCC_Principal cells\",\n",
    "    \"RCC_Tumor cells 1\", \"RCC_Tumor cells 2\", \"RCC_Tumor cells 3\", \"RCC_tAL of LOH\"],\n",
    "\"Endothelial\" : [ \"MF_pDCs\", \"MF_Arterial_EC\", \"MF_Erythroid_cells\", \"MF_Late_Erythroid_Prog\",\n",
    "    \"MF_Sinusoidal_EC\", \"RCC_AVR\", \"RCC_DVR\", \"RCC_Tumor AVR-like vasculature\",\n",
    "    \"RCC_Tumor vasculature 1\", \"RCC_Tumor vasculature 2\", \"RCC_Tumor vasculature 3\", \"RCC_Tumor vasculature 4\"],\n",
    "\"Stromal\" : [\"MF_Adipo_CAR\", \"MF_Mature_MKs\", \"MF_OLCs\", \"MF_Osteoblasts\", \"RCC_Mesangial/vSMCs\",\n",
    "    \"RCC_Myofibroblasts\", \"RCC_vSMCs\"],\n",
    "\"Mast_cells\" : [\"MF_Mast_cells\", \"RCC_Mast cells\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting subgraphs\n",
    "subgraphs = [g_comb.subgraph(cluster) for cluster in clusters_comb]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(subgraphs), figsize=(10*len(subgraphs), 10))\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import colors as mcolors\n",
    "\n",
    "cluster_names = list(cluster_dict.keys())\n",
    "cluster_names = [\"Myeloids\", \"Tumor/prog\", \"Endothelial\", \"Lymphocytes\", \"Stromal\", \"Mast cells\"]\n",
    "\n",
    "# pick a colormap\n",
    "cmap = cm.get_cmap(\"tab10\", len(subgraphs))  # up to 10 distinct colors\n",
    "\n",
    "colors = [cmap(i) for i in range(len(subgraphs))]\n",
    "edge_alpha = 0.4  # edge transparency\n",
    "edge_color = mcolors.to_rgba(\"black\", alpha=edge_alpha)\n",
    "\n",
    "\n",
    "for i, (sg, cluster_name) in enumerate(zip(subgraphs, cluster_names)):\n",
    "    filename = f\"{cluster_name.replace('/', '_').replace(' ', '_')}_igraph.png\"\n",
    "    layout = sg.layout(\"fr\")  # Fruchterman-Reingold (force-directed)\n",
    "    color = colors[i]\n",
    "    ig.plot(\n",
    "        sg,\n",
    "        #target=axes[i],\n",
    "        layout=layout,\n",
    "        vertex_size=30,\n",
    "        vertex_label=sg.vs[\"name\"],\n",
    "        vertex_label_size = 15,\n",
    "        vertex_label_dist = 2,\n",
    "        target=filename,\n",
    "        edge_width=[w for w in sg.es[\"weight\"]],\n",
    "        vertex_color=[color] * sg.vcount(),\n",
    "        edge_color=[edge_color] * sg.ecount(),\n",
    "        margin=20\n",
    "    )\n",
    "    axes[i].set_title(f\"{cluster_name}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#for clust in clusters_comb:\n",
    "#    ig.plot(clust, layout = layout, vertex_size=30, edge_width=[w[\"weight\"] for w in g_comb.es], edge_alpha= 0.4, target=\"clustered_igraph_quant.png\", \n",
    "#        bbox = (1500,1500), margin = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving subpllots formatted (plotting subgraphs v2)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(subgraphs), figsize=(12*len(subgraphs), 12))\n",
    "\n",
    "cluster_names = [\"Myeloids\", \"Tumor/prog\", \"Endothelial\", \"Lymphocytes\", \"Stromal\", \"Mast cells\"]\n",
    "\n",
    "\n",
    "cmap = cm.get_cmap(\"tab10\", len(subgraphs))\n",
    "colors = [cmap(i) for i in range(len(subgraphs))]\n",
    "\n",
    "\n",
    "edge_alpha = 0.4\n",
    "edge_color = mcolors.to_rgba(\"black\", alpha=edge_alpha)\n",
    "\n",
    "def vertex_color_by_prefix(name):\n",
    "    if name.startswith(\"RCC\"):\n",
    "        return \"#FF944C\"  \n",
    "    elif name.startswith(\"MF\"):\n",
    "        return \"#7FCE7B\"\n",
    "    else:\n",
    "        return \"#CCCCCC\"  # \n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor=\"#FF944C\", edgecolor=\"black\", label=\"RCC\"),\n",
    "    Patch(facecolor=\"#7FCE7B\", edgecolor=\"black\", label=\"MF\")\n",
    "]\n",
    "\n",
    "for i, (sg, cluster_name) in enumerate(zip(subgraphs, cluster_names)):\n",
    "    layout = sg.layout(\"fr\")  \n",
    "\n",
    "\n",
    "    vertex_colors = [vertex_color_by_prefix(v[\"name\"]) for v in sg.vs]\n",
    "    filename = f\"{cluster_name.replace('/', '_').replace(' ', '_')}_igraph.png\"\n",
    "\n",
    "\n",
    "    ig.plot(\n",
    "        sg,\n",
    "        target=filename,\n",
    "        layout=layout,\n",
    "        vertex_size=20,\n",
    "        vertex_label=sg.vs[\"name\"],\n",
    "        vertex_label_size=15,\n",
    "        vertex_label_dist=2,\n",
    "        edge_width=sg.es[\"weight\"],\n",
    "        vertex_color=vertex_colors,\n",
    "        edge_color=[edge_color] * sg.ecount(),\n",
    "        margin=80\n",
    "    )\n",
    "\n",
    "\n",
    "    img = plt.imread(filename)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "   \n",
    "    ax.legend(\n",
    "        handles=legend_elements,\n",
    "        loc=\"upper left\",\n",
    "        frameon=False,\n",
    "        fontsize=12,\n",
    "        ncol=1\n",
    "    )\n",
    "\n",
    "    outname = f\"{cluster_name.replace('/', '_').replace(' ', '_')}_igraph_with_legend.png\"\n",
    "    plt.savefig(outname, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyvoi bc i had import issues (chatgpt version delivers same results)\n",
    "import numpy as np\n",
    "\n",
    "import numpy.typing as npt\n",
    "import warnings\n",
    "\n",
    "\n",
    "def VI_np(labels1,labels2,return_split_merge=False):\n",
    "    assert len(labels2)==len(labels1)\n",
    "    size=len(labels2)\n",
    "\n",
    "    mutual_labels=(labels1.astype(np.uint64)<<32)+labels2.astype(np.uint64)\n",
    "\n",
    "    sm_unique,sm_inverse,sm_counts=np.unique(labels2,return_inverse=True,return_counts=True)\n",
    "    fm_unique,fm_inverse,fm_counts=np.unique(labels1,return_inverse=True,return_counts=True)\n",
    "    _,mutual_inverse,mutual_counts=np.unique(mutual_labels,return_inverse=True,return_counts=True)\n",
    "\n",
    "    terms_mutual = -np.log(mutual_counts/size)*mutual_counts/size\n",
    "    terms_mutual_per_count=terms_mutual[mutual_inverse]/mutual_counts[mutual_inverse]\n",
    "    terms_sm = -np.log(sm_counts/size)*sm_counts/size\n",
    "    terms_fm = -np.log(fm_counts/size)*fm_counts/size\n",
    "    if not return_split_merge:\n",
    "        terms_mutual_sum=np.sum(terms_mutual_per_count)\n",
    "        vi_split=terms_mutual_sum-terms_sm.sum()\n",
    "        vi_merge=terms_mutual_sum-terms_fm.sum()\n",
    "        vi=vi_split+vi_merge\n",
    "        return vi,vi_split,vi_merge\n",
    "\n",
    "    vi_split_each=np.zeros(len(sm_unique))\n",
    "    np.add.at(vi_split_each,sm_inverse,terms_mutual_per_count)\n",
    "    vi_split_each-=terms_sm\n",
    "    vi_merge_each=np.zeros(len(fm_unique))\n",
    "    np.add.at(vi_merge_each,fm_inverse,terms_mutual_per_count)\n",
    "    vi_merge_each-=terms_fm\n",
    "\n",
    "    vi_split=np.sum(vi_split_each)\n",
    "    vi_merge=np.sum(vi_merge_each)\n",
    "    vi=vi_split+vi_merge\n",
    "\n",
    "    i_splitters=np.argsort(vi_split_each)[::-1]\n",
    "    i_mergers=np.argsort(vi_merge_each)[::-1]\n",
    "\n",
    "    vi_split_sorted=vi_split_each[i_splitters]\n",
    "    vi_merge_sorted=vi_merge_each[i_mergers]\n",
    "\n",
    "    splitters=np.stack([vi_split_sorted,sm_unique[i_splitters]],axis=1)\n",
    "    mergers=np.stack([vi_merge_sorted,fm_unique[i_mergers]],axis=1)\n",
    "    return vi,vi_split,vi_merge,splitters,mergers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores validating that clustering stayed consistent\n",
    "resolutions = np.linspace(0.15, 0.35, 20) \n",
    "results = []\n",
    "\n",
    "\n",
    "def get_partition_labels(g, resolution, seed):\n",
    "    partition = leidenalg.find_partition(\n",
    "        g_comb,\n",
    "        leidenalg.CPMVertexPartition,\n",
    "        resolution_parameter=resolution,\n",
    "        weights=g_comb.es[\"weight\"],\n",
    "        seed=seed+500\n",
    "    )\n",
    "    return partition.membership\n",
    "\n",
    "runs = 100\n",
    "\n",
    "for r in resolutions:\n",
    "    aris = []\n",
    "    nmi = []\n",
    "    partition = leidenalg.find_partition(\n",
    "        g_comb,\n",
    "        leidenalg.CPMVertexPartition,\n",
    "        weights=g_comb.es[\"weight\"],\n",
    "        resolution_parameter=r\n",
    "    )\n",
    "    modularity = partition.modularity\n",
    "\n",
    "    labels = [get_partition_labels(g_comb, r, seed=i) for i in range(runs)]\n",
    "\n",
    "    for i in range(runs):\n",
    "        for j in range(i+1, runs):\n",
    "            aris.append(adjusted_rand_score(labels[i], labels[j]))\n",
    "            nmi.append(normalized_mutual_info_score(labels[i], labels[j]))\n",
    "            vi,vi_split,vi_merge=VI_np(np.array(labels[i]), np.array(labels[j]),return_split_merge=False)\n",
    "    results.append((r, np.mean(aris), np.mean(nmi), np.mean(vi), modularity, len(partition)))\n",
    "\n",
    "for r, aris_mean, nmi, vi, mod, k in results:\n",
    "    print(f\"Resolution {r:.2f}: mean ARI = {aris_mean:.3f}; mean NMI = {nmi:.3f}; mean VI = {vi:.3f}; modularity={mod:.3f}; n_clusters={k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full feature matrix\n",
    "\n",
    "#RCC = pd.read_csv(\"/home/larissa/Documents/Masterarbeit/RCC_results/liana_log_norm/no_subsets/T_lr_ready.csv\", index_col=0)\n",
    "#MF = pd.read_csv(\"/home/larissa/Documents/Masterarbeit/MF_results/liana/all_celltypes/MF_lr_ready.csv\", index_col=0)\n",
    "\n",
    "#RCC = pd.read_csv(\"D:\\\\studium\\\\Masterarbeit\\\\RCC_results\\\\liana_log_norm\\\\no_subsets\\\\T_lr_ready.csv\", index_col=0)\n",
    "#MF = pd.read_csv(\"D:\\\\studium\\\\Masterarbeit\\\\MF_results\\\\liana\\\\all_celltypes\\\\MF_lr_ready.csv\", index_col=0)\n",
    "\n",
    "RCC = pd.read_csv(\"C:\\\\Users\\\\laris\\\\Documents\\\\Studium\\\\Masterarbeit\\\\RCC_results\\\\liana_log_norm\\\\no_subset_glom_removed\\\\T_lr_ready.csv\", index_col = 0)\n",
    "MF = pd.read_csv(\"C:\\\\Users\\\\laris\\\\Documents\\\\Studium\\\\Masterarbeit\\\\MF_results\\\\liana\\\\all_celltypes\\\\MF_lr_ready.csv\", index_col = 0)\n",
    "\n",
    "RCC[\"lr_pair\"] = RCC[\"gene_A\"] + \"_\" + RCC[\"gene_B\"]\n",
    "MF[\"lr_pair\"] = MF[\"gene_A\"] + \"_\" + MF[\"gene_B\"]\n",
    "\n",
    "RCC[\"cluster_pair\"] = RCC[\"source\"] + \"@\" + RCC[\"target\"]\n",
    "MF[\"cluster_pair\"] = MF[\"source\"] + \"@\" + MF[\"target\"]\n",
    "\n",
    "#lr_threshold = 0.0\n",
    "RCC_filtered = RCC[(RCC[\"lr_means\"] >= 0.0) | (RCC[\"lr_means\"] <=  -abs(0.0))]\n",
    "MF_filtered = MF[(MF[\"lr_means\"] >= 0.0) | (MF[\"lr_means\"] <= -abs(0.0))]\n",
    "\n",
    "RCC_grouped = (RCC_filtered.groupby(\"cluster_pair\")\n",
    "               .apply(lambda df: dict(zip(df[\"lr_pair\"], df[\"lr_means\"])))\n",
    "               .to_dict())\n",
    "\n",
    "MF_grouped = (MF_filtered.groupby(\"cluster_pair\")\n",
    "              .apply(lambda df: dict(zip(df[\"lr_pair\"], df[\"lr_means\"])))\n",
    "              .to_dict())\n",
    "\n",
    "def weighted_jaccard_dict(dict1, dict2):\n",
    "    all_keys = set(dict1.keys()).union(dict2.keys())\n",
    "    vec1 = np.array([dict1.get(i, 0.0) for i in all_keys])\n",
    "    vec2 = np.array([dict2.get(i, 0.0) for i in all_keys])\n",
    "    return np.minimum(vec1, vec2).sum() / np.maximum(vec1, vec2).sum() if np.maximum(vec1, vec2).sum() != 0 else 0.0\n",
    "\n",
    "# pairwise similarity\n",
    "results = []\n",
    "for MF_cluster, MF_dict in MF_grouped.items():\n",
    "    for RCC_cluster, RCC_dict in RCC_grouped.items():\n",
    "        score = weighted_jaccard_dict(MF_dict, RCC_dict)\n",
    "        results.append((MF_cluster, RCC_cluster, score))\n",
    "        results.append((RCC_cluster, MF_cluster, score))\n",
    "\n",
    "for MF_cluster, MF_dict in MF_grouped.items():\n",
    "    for MF_cluster2, MF_dict2 in MF_grouped.items():\n",
    "        score = weighted_jaccard_dict(MF_dict, MF_dict2)\n",
    "        results.append((MF_cluster, MF_cluster2, score))\n",
    "\n",
    "for RCC_cluster, RCC_dict in RCC_grouped.items():\n",
    "    for RCC_cluster2, RCC_dict2 in RCC_grouped.items():\n",
    "        score = weighted_jaccard_dict(RCC_dict, RCC_dict2)\n",
    "        results.append((RCC_cluster, RCC_cluster2, score))\n",
    "\n",
    "\n",
    "\n",
    "jaccard_df = pd.DataFrame(results, columns=[\"cluster_pair_1\", \"cluster_pair_2\", \"weighted_jaccard\"])\n",
    "#jaccard_df.to_csv(\"jaccard_score_weighted_cluster_panirs_thresholded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full feature heatmap\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "#same clusters are not 1 but do they need to be for visualization? no\n",
    "\n",
    "# pivot to square matrix\n",
    "\n",
    "sim_matrix = jaccard_df.pivot(\n",
    "    index=\"cluster_pair_1\", columns=\"cluster_pair_2\", values=\"weighted_jaccard\"\n",
    ")\n",
    "\n",
    "dist_matrix = 1 - sim_matrix\n",
    "\n",
    "#flatten to condensed distance matrix\n",
    "condensed_dist = squareform(dist_matrix.values, checks=False)\n",
    "\n",
    "# hierarchical clustering \n",
    "linkage_matrix = linkage(condensed_dist, method='average')\n",
    "\n",
    "clust_map = sns.clustermap(sim_matrix, row_linkage=linkage_matrix, col_linkage=linkage_matrix, row_cluster=True, col_cluster=True, figsize=(30,30), cmap=\"viridis\")\n",
    "#sim_matrix.to_csv(\"sim_matrix_jaccard.csv\")\n",
    "clust_map.savefig(\"jaccard_score_dist_linkage_full_feature_average_linkage.png\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full feature without axis ticks \n",
    "\n",
    "#\n",
    "sim_matrix = jaccard_df.pivot(\n",
    "    index=\"cluster_pair_1\", columns=\"cluster_pair_2\", values=\"weighted_jaccard\"\n",
    ")\n",
    "\n",
    "dist_matrix = 1 - sim_matrix\n",
    "\n",
    "condensed_dist = squareform(dist_matrix.values, checks=False)\n",
    "\n",
    "linkage_matrix = linkage(condensed_dist, method='average')\n",
    "\n",
    "clust_map = sns.clustermap(\n",
    "    sim_matrix,\n",
    "    row_linkage=linkage_matrix,\n",
    "    col_linkage=linkage_matrix,\n",
    "    row_cluster=True,\n",
    "    col_cluster=True,\n",
    "    figsize=(30, 30),\n",
    "    cmap=\"viridis\",\n",
    "    cbar_kws={\"shrink\": 0.4}\n",
    "\n",
    "\n",
    "clust_map.ax_heatmap.set_xticks([])\n",
    "clust_map.ax_heatmap.set_yticks([])\n",
    "\n",
    "\n",
    "clust_map.ax_row_dendrogram.set_ylabel(\"Clusters\", fontsize=18)\n",
    "clust_map.ax_col_dendrogram.set_xlabel(\"Clusters\", fontsize=18)\n",
    "\n",
    "\n",
    "clust_map.cax.tick_params(labelsize=18)\n",
    "\n",
    "clust_map.savefig(\"jaccard_score_dist_linkage_full_feature_average_linkage.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prince\n",
    "\n",
    "dist_matrix_rect = 1 - heatrmap_df\n",
    "\n",
    "ca = prince.CA(n_components=2)\n",
    "ca = ca.fit(dist_matrix)\n",
    "\n",
    "row_coords = ca.row_coordinates(dist_matrix)\n",
    "col_coords = ca.column_coordinates(dist_matrix)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(row_coords[0], row_coords[1], c='red', label='Rows')\n",
    "plt.scatter(col_coords[0], col_coords[1], c='blue', label='Columns')\n",
    "\n",
    "#labels\n",
    "#or i, txt in enumerate(dist_matrix_rect.index):\n",
    " #   plt.annotate(txt, (row_coords[0][i], row_coords[1][i]), color='red')\n",
    "\n",
    "#for i, txt in enumerate(dist_matrix_rect.columns):#\n",
    " #   plt.annotate(txt, (col_coords[0][i], col_coord#s[1][i]), color='blue')\n",
    "\n",
    "\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT full feature (sparse matrix)\n",
    "\n",
    "#same clusters are not 1 but do they need to be for visualization? no\n",
    "\n",
    "mat1 = jaccard_df_comp.pivot(\n",
    "    index=\"RCC_cluster\", columns=\"MF_cluster\", values=\"weighted_jaccard\"\n",
    ")\n",
    "mat2 = jaccard_df_comp.pivot(\n",
    "    index=\"MF_cluster\", columns=\"RCC_cluster\", values=\"weighted_jaccard\"\n",
    ")\n",
    "\n",
    "mat3 = pd.concat([mat1, mat2])\n",
    "#mat3 = mat3.sort_index(axis=0).sort_index(axis=1)\n",
    "mat3 = mat3.fillna(0)\n",
    "dist_matrix = 1 - mat3\n",
    "\n",
    "\n",
    "\n",
    "linkage_matrix = linkage(condensed_dist, method='average')\n",
    "\n",
    "clust_map = sns.clustermap(mat3, row_linkage=linkage_matrix, col_linkage=linkage_matrix, row_cluster=True, col_cluster=True, figsize=(100,100), cmap=\"viridis\")\n",
    "#sim_matrix.to_csv(\"sim_matrix_jaccard.csv\")\n",
    "clust_map.ax_heatmap.set_xticks([])\n",
    "clust_map.ax_heatmap.set_yticks([])\n",
    "\n",
    "clust_map.ax_row_dendrogram.set_ylabel(\"Clusters\", fontsize=18)\n",
    "clust_map.ax_col_dendrogram.set_xlabel(\"Clusters\", fontsize=18)\n",
    "\n",
    "\n",
    "clust_map.cax.tick_params(labelsize=18)\n",
    "\n",
    "clust_map.savefig(\"jaccard_score_dist_linkage_sparse_mat_average_linkage.png\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
